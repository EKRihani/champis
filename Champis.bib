
@inproceedings{guyon_scaling_1997,
	title = {A {Scaling} {Law} for the {Validation}-{Set} {Training}-{Set} {Size} {Ratio}},
	url = {https://www.semanticscholar.org/paper/A-Scaling-Law-for-the-Validation-Set-Training-Set-Subramanian/452e6c05d46e061290fefff8b46d0ff161998677},
	abstract = {We address the problem of determining what fraction of the training set should be reserved as development test set or validation set. We determine that the ratio of the validation set size over the training set size scales like the square root of two complexity parameters: the complexity of the second level of inference (minimizing the validation error) over the complexity of the rst level of inference (minimizing the error rate on the training set).},
	urldate = {2023-02-15},
	author = {Guyon, Isabelle},
	year = {1997},
	file = {Subramanian - 1997 - A Scaling Law for the Validation-Set Training-Set .pdf:/home/ekr/Zotero/storage/JIBUUSH5/Subramanian - 1997 - A Scaling Law for the Validation-Set Training-Set .pdf:application/pdf},
}

@article{mak_support_2018,
	title = {Support points},
	volume = {46},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Support-points/10.1214/17-AOS1629.full},
	doi = {10.1214/17-AOS1629},
	abstract = {This paper introduces a new way to compact a continuous probability distribution \$F\$ into a set of representative points called support points. These points are obtained by minimizing the energy distance, a statistical potential measure initially proposed by Székely and Rizzo [InterStat 5 (2004) 1–6] for testing goodness-of-fit. The energy distance has two appealing features. First, its distance-based structure allows us to exploit the duality between powers of the Euclidean distance and its Fourier transform for theoretical analysis. Using this duality, we show that support points converge in distribution to \$F\$, and enjoy an improved error rate to Monte Carlo for integrating a large class of functions. Second, the minimization of the energy distance can be formulated as a difference-of-convex program, which we manipulate using two algorithms to efficiently generate representative point sets. In simulation studies, support points provide improved integration performance to both Monte Carlo and a specific quasi-Monte Carlo method. Two important applications of support points are then highlighted: (a) as a way to quantify the propagation of uncertainty in expensive simulations and (b) as a method to optimally compact Markov chain Monte Carlo (MCMC) samples in Bayesian computation.},
	number = {6A},
	urldate = {2023-02-15},
	journal = {The Annals of Statistics},
	author = {Mak, Simon and Joseph, V. Roshan},
	month = dec,
	year = {2018},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62E17, Bayesian computation, energy distance, Monte Carlo, numerical integration, quasi-Monte Carlo, representative points},
	pages = {2562--2592},
	file = {Full Text PDF:/home/ekr/Zotero/storage/5UGUPJMD/Mak et Joseph - 2018 - Support points.pdf:application/pdf},
}

@article{joseph_split_2022,
	title = {{SPlit}: {An} {Optimal} {Method} for {Data} {Splitting}},
	volume = {64},
	issn = {0040-1706},
	shorttitle = {{SPlit}},
	url = {https://doi.org/10.1080/00401706.2021.1921037},
	doi = {10.1080/00401706.2021.1921037},
	abstract = {In this article, we propose an optimal method referred to as SPlit for splitting a dataset into training and testing sets. SPlit is based on the method of support points (SP), which was initially developed for finding the optimal representative points of a continuous distribution. We adapt SP for subsampling from a dataset using a sequential nearest neighbor algorithm. We also extend SP to deal with categorical variables so that SPlit can be applied to both regression and classification problems. The implementation of SPlit on real datasets shows substantial improvement in the worst-case testing performance for several modeling methods compared to the commonly used random splitting procedure.},
	number = {2},
	urldate = {2023-02-15},
	journal = {Technometrics},
	author = {Joseph, V. Roshan and Vakayil, Akhil},
	month = apr,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2021.1921037},
	keywords = {Cross-validation, Quasi-Monte Carlo, Testing, Training, Validation},
	pages = {166--176},
	file = {Full Text PDF:/home/ekr/Zotero/storage/YA29N3TY/Joseph et Vakayil - 2022 - SPlit An Optimal Method for Data Splitting.pdf:application/pdf},
}

@article{joseph_optimal_2022,
	title = {Optimal ratio for data splitting},
	volume = {15},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11583},
	doi = {10.1002/sam.11583},
	abstract = {It is common to split a dataset into training and testing sets before fitting a statistical or machine learning model. However, there is no clear guidance on how much data should be used for training and testing. In this article, we show that the optimal training/testing splitting ratio is p:1\$\$ {\textbackslash}sqrtp:1 \$\$, where p\$\$ p \$\$ is the number of parameters in a linear regression model that explains the data well.},
	language = {en},
	number = {4},
	urldate = {2023-02-15},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Joseph, V. Roshan},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11583},
	keywords = {testing, training, validation},
	pages = {531--538},
	file = {Snapshot:/home/ekr/Zotero/storage/EASBXNBQ/sam.html:text/html;Full Text PDF:/home/ekr/Zotero/storage/9CXHYC28/Joseph - 2022 - Optimal ratio for data splitting.pdf:application/pdf},
}

@misc{brownlee_what_2017,
	title = {What is the {Difference} {Between} {Test} and {Validation} {Datasets}?},
	url = {https://machinelearningmastery.com/difference-test-validation-datasets/},
	abstract = {A validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters. The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased […]},
	language = {en-US},
	urldate = {2023-02-14},
	journal = {MachineLearningMastery.com},
	author = {Brownlee, Jason},
	month = jul,
	year = {2017},
	file = {Snapshot:/home/ekr/Zotero/storage/QDMPPKQE/difference-test-validation-datasets.html:text/html},
}

@book{johnson_continuous_1995,
	address = {New York [etc},
	edition = {2nd ed.},
	series = {Wiley series in probability and mathematical statistics {Applied} probability and statistics},
	title = {Continuous univariate distributions, volume 2},
	volume = {2},
	isbn = {978-0-471-58494-0},
	language = {eng},
	publisher = {John Wiley \& sons},
	author = {Johnson, Norman Lloyd},
	year = {1995},
	keywords = {Distribution (théorie des probabilités)},
	file = {Johnson - 1995 - Continuous univariate distributions, volume 2.pdf:/home/ekr/Zotero/storage/CZFQ8I2H/Johnson - 1995 - Continuous univariate distributions, volume 2.pdf:application/pdf},
}

@article{porter_hyphal_2022,
	title = {Hyphal systems and their effect on the mechanical properties of fungal sporocarps},
	volume = {145},
	issn = {1742-7061},
	url = {https://www.sciencedirect.com/science/article/pii/S1742706122002161},
	doi = {10.1016/j.actbio.2022.04.011},
	abstract = {Little is known about the mechanical and material properties of hyphae, the single constituent material of Agaricomycetes fungi, despite a growing interest in fungus-based materials. In the Agaricomycetes (the mushrooms and allies), there are three types of hyphae that make up sporocarps: generative, skeletal, and ligative. All filamentous Agaricomycetes can be categorized into one of three categories of hyphal systems that compose them: monomitic, dimitic, and trimitic. Monomitic systems have only generative hyphae. Dimitic systems have generative and either skeletal (most common) or ligative. Trimitic systems are composed of all three kinds of hyphae. SEM imaging, compression testing, and theoretical modeling were used to characterize the material and mechanical properties of representative monomitic, dimitic, and trimitic sporocarps. Compression testing revealed an increase in the compression modulus and compressive strength with the addition of more hyphal types (monomitic to dimitic and dimitic to trimitic). The mesostructure of the trimitic sporocarp was tested and modeled, suggesting that the difference in properties between the solid material and the microtubule mesostructure is a result of differences in structure and not material. Theoretical modeling was completed to estimate the mechanical properties of the individual types of hyphae and showed that skeletal hyphae make the largest contribution to mechanical properties of fungal sporocarps. Understanding the contributions of the different types of hyphae may help in the design and application of fungi-based or bioinspired materials.
Statement of Significance
This research studies the material and mechanical properties of fungal sporocarps and their hyphae, the single constituent material of Agaricomycetes fungi. Though some work has been done on fungal hyphae, this research studies hyphae in context of the three hyphal systems found in Agaricomycetes fungi and estimates the properties of the hyphal filaments, which has not been done previously. This characterization was performed by analyzing the structures and mechanical properties of fungal sporocarps and calculating the theoretical mechanical properties of their hyphae. This data and the resulting conclusions may lead to a better design and implementation process of fungi-based materials in various applications using the properties now known or calculated.},
	language = {en},
	urldate = {2023-02-11},
	journal = {Acta Biomaterialia},
	author = {Porter, Debora Lyn and Naleway, Steven E.},
	month = jun,
	year = {2022},
	keywords = {Fungi, Hyphae, Material science, Mechanical properties},
	pages = {272--282},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/QSPG4PXT/S1742706122002161.html:text/html},
}

@article{noauthor_insights_2008,
	title = {Insights on the mechanics of hyphal growth},
	volume = {22},
	issn = {1749-4613},
	url = {https://www.sciencedirect.com/science/article/abs/pii/S1749461308000195},
	doi = {10.1016/j.fbr.2008.05.002},
	abstract = {This article reviews recent progress in understanding the mechanics of hyphal growth. The pressurization of hyphae by osmosis is often considered an i…},
	language = {en},
	number = {2},
	urldate = {2023-02-11},
	journal = {Fungal Biology Reviews},
	month = may,
	year = {2008},
	note = {Publisher: Elsevier},
	keywords = {Cytoskeleton, Invasive growth, Pollen tubes, Pulsatile growth, Tip growth, Turgor pressure},
	pages = {71--76},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/UWBYPAQB/S1749461308000195.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/EJAMHCS2/Money - 2008 - Insights on the mechanics of hyphal growth.pdf:application/pdf;Snapshot:/home/ekr/Zotero/storage/TIIKMYEL/S1749461308000195.html:text/html},
}

@article{schlimmer_mushroom_1987,
	edition = {University of California},
	title = {Mushroom {Data} {Set}},
	url = {https://archive.ics.uci.edu/ml/datasets/Mushroom},
	language = {English},
	author = {Schlimmer, Jeff},
	year = {1987},
}

@book{kuhn_caret_nodate,
	title = {The caret {Package}},
	url = {https://topepo.github.io/caret/index.html},
	abstract = {Documentation for the caret package.},
	urldate = {2022-12-11},
	author = {Kuhn, Max},
	file = {Snapshot:/home/ekr/Zotero/storage/8GNIME6R/index.html:text/html},
}

@article{bergstra_random_nodate,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	pages = {25},
	file = {Bergstra et Bengio - Random Search for Hyper-Parameter Optimization.pdf:/home/ekr/Zotero/storage/2M2SGP7Z/Bergstra et Bengio - Random Search for Hyper-Parameter Optimization.pdf:application/pdf},
}

@book{gramacy_surrogates_nodate,
	title = {Surrogates},
	url = {https://bookdown.org/rbg/surrogates/#front-cover},
	abstract = {Surrogates: a new graduate level textbook on topics lying at the interface between machine learning, spatial statistics, computer simulation, meta-modeling (i.e., emulation), and design of experiments. Gaussian process emphasis facilitates flexible nonparametric and nonlinear modeling, with applications to uncertainty quantification, sensitivity analysis, calibration of computer models to field data, sequential design and (blackbox) optimization under uncertainty. Presentation targets numerically competent scientists in the engineering, physical, and biological sciences. Treatment includes historical perspective and canonical examples, but primarily concentrates on modern statistical methods, computation and implementation in R at modern scale. Rmarkdown facilitates a fully reproducible tour complete with motivation from, application to, and illustration with, compelling real-data examples.},
	urldate = {2022-12-11},
	author = {Gramacy, Robert B.},
	file = {Snapshot:/home/ekr/Zotero/storage/GJP5VIHW/surrogates.html:text/html},
}

@article{wagner_mushroom_2021,
	title = {Mushroom data creation, curation, and simulation to support classification tasks},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-87602-3},
	doi = {10.1038/s41598-021-87602-3},
	abstract = {Predicting if a set of mushrooms is edible or not corresponds to the task of classifying them into two groups—edible or poisonous—on the basis of a classification rule. To support this binary task, we have collected the largest and most comprehensive attribute based data available. In this work, we detail the creation, curation and simulation of a data set for binary classification. Thanks to natural language processing, the primary data are based on a text book for mushroom identification and contain 173 species from 23 families. While the secondary data comprise simulated or hypothetical entries that are structurally comparable to the 1987 data, it serves as pilot data for classification tasks. We evaluated different machine learning algorithms, namely, naive Bayes, logistic regression, and linear discriminant analysis (LDA), and random forests (RF). We found that the RF provided the best results with a five-fold Cross-Validation accuracy and F2-score of 1.0 (\$\${\textbackslash}mu =1\$\$, \$\${\textbackslash}sigma =0\$\$), respectively. The results of our pilot are conclusive and indicate that our data were not linearly separable. Unlike the 1987 data which showed good results using a linear decision boundary with the LDA. Our data set contains 23 families and is the largest available. We further provide a fully reproducible workflow and provide the data under the FAIR principles.},
	language = {en},
	number = {1},
	urldate = {2022-12-10},
	journal = {Scientific Reports},
	author = {Wagner, Dennis and Heider, Dominik and Hattab, Georges},
	month = apr,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Classification and taxonomy, Computational models, Data integration, Data mining, Data processing, Machine learning, Quality control, Scientific data},
	pages = {8134},
	file = {Full Text PDF:/home/ekr/Zotero/storage/68TDVMV2/Wagner et al. - 2021 - Mushroom data creation, curation, and simulation t.pdf:application/pdf},
}

@misc{courtecuisse_initiation_2020,
	title = {Initiation à la reconnaissance des champignons du {Nord} de la {France} - {Clé} pour la détermination des espèces les plus fréquentes},
	shorttitle = {Clé de détermination des champignons du {Nord} de la {France}},
	language = {français},
	publisher = {Département des Sciences Végétales et Fongiques, Faculté de Pharmacie de Lille},
	author = {Courtecuisse, Régis and Moreau, Pierre-Arthur and Welti, Stéphane},
	year = {2020},
}

@book{courtecuisse_champignons_2013,
	series = {Guides {Delachaux}},
	title = {Champignons de {France} et d'{Europe}},
	isbn = {978-2-603-02038-8},
	language = {français},
	publisher = {Delachaux et Niestlé},
	author = {Courtecuisse, Régis and Duhem, Bernard},
	year = {2013},
}

@book{courtecuisse_cle_1986,
	title = {Clé de détermination macroscopique des champignons supérieurs des régions du {Nord} de la {France}},
	language = {français},
	publisher = {Société mycologique du Nord de la France},
	author = {Courtecuisse, Régis},
	year = {1986},
}

@book{r_core_team_r_2022,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	language = {Anglais},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2022},
}

@article{santiago_construction_2012,
	title = {Construction of space-filling designs using {WSP} algorithm for high dimensional spaces},
	volume = {113},
	issn = {01697439},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169743911001195},
	doi = {10.1016/j.chemolab.2011.06.003},
	abstract = {In the computer experiments setting, if the relationship between the response and the inputs is unknown, then the purpose is to use designs that spread the points at which the response is observed evenly throughout the region. These designs are called space-ﬁlling designs (SFD) and the most known are Latin Hypercubes (random, orthogonal, optimized) and low discrepancy sequences. But, simulation codes becoming more and more complex, high dimensional optimal designs are needed to study a high number of parameters (more than 20 parameters) and the construction proves difﬁcult. The aim of this study is to explore a construction method of new space-ﬁlling designs for high dimensional spaces. After a short presentation of the criteria considered to quantify the intrinsic quality of the designs, the generation of these designs using WSP algorithm is presented. As the ﬁrst step consists in generating candidate points, the inﬂuence of the initial set of points is investigated in dimension 20 and the ﬁnal designs are compared with others space-ﬁlling designs. Then, designs are proposed in dimension 20, 30, 40 and 50 and the study of the intrinsic quality of these new space-ﬁlling designs highlights the robustness of this generation method in high dimensional spaces.},
	language = {en},
	urldate = {2023-03-04},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Santiago, J. and Claeys-Bruno, M. and Sergent, M.},
	month = apr,
	year = {2012},
	pages = {26--31},
	file = {Santiago et al. - 2012 - Construction of space-filling designs using WSP al.pdf:/home/ekr/Zotero/storage/K4BDP67I/Santiago et al. - 2012 - Construction of space-filling designs using WSP al.pdf:application/pdf},
}

@article{youden_index_1950,
	title = {Index for rating diagnostic tests},
	volume = {3},
	issn = {1097-0142},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-0142%281950%293%3A1%3C32%3A%3AAID-CNCR2820030106%3E3.0.CO%3B2-3},
	doi = {10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3},
	language = {en},
	number = {1},
	urldate = {2023-03-04},
	journal = {Cancer},
	author = {Youden, W. J.},
	year = {1950},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-0142\%281950\%293\%3A1\%3C32\%3A\%3AAID-CNCR2820030106\%3E3.0.CO\%3B2-3},
	pages = {32--35},
	file = {Full Text PDF:/home/ekr/Zotero/storage/SYG6VC9F/Youden - 1950 - Index for rating diagnostic tests.pdf:application/pdf;Snapshot:/home/ekr/Zotero/storage/KEGXCE5R/1097-0142(1950)3132AID-CNCR28200301063.0.html:text/html},
}

@article{rucker_summary_2010,
	title = {Summary {ROC} curve based on a weighted {Youden} index for selecting an optimal cutpoint in meta-analysis of diagnostic accuracy},
	volume = {29},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3937},
	doi = {10.1002/sim.3937},
	abstract = {Established approaches for analyzing meta-analyses of diagnostic accuracy model the bivariate distribution of the observed pairs of specificity Sp and sensitivity Se, thus accounting for across-study correlation. However, it is still a matter of debate how to define a summary ROC (SROC) curve. It was recently pointed out that the SROC curve is in principle unidentifiable if only one (Sp, Se) pair per study is known. We evaluate an alternative approach, modeling the study-specific ROC curves based on the assumption of linearity in logit space. A setting is considered in which the pair (Sp, Se) that is selected for publication in a particular study maximizes a weighted Youden index λSe+(1−λ)Sp with a given weight λ.This leads to a fixed slope (1−λ)/λ of the ROC curve in (1−Sp, Se), equivalent to a slope of (1−λ)Sp(1−Sp)/(λSe(1−Se)) for the corresponding straight line in logit space. While the slope depends on the variance ratio of the underlying distributions, the intercept is a function of the mean difference. Our approach leads in a natural way to a new, model-based proposal for a summary ROC curve. It is illustrated using an example from the literature. Copyright © 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {30},
	urldate = {2023-03-04},
	journal = {Statistics in Medicine},
	author = {Rücker, Gerta and Schumacher, Martin},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3937},
	keywords = {meta-analysis, diagnostic accuracy, summary ROC curve, Youden index},
	pages = {3069--3078},
	file = {Full Text PDF:/home/ekr/Zotero/storage/YFIIMPHV/Rücker et Schumacher - 2010 - Summary ROC curve based on a weighted Youden index.pdf:application/pdf;Snapshot:/home/ekr/Zotero/storage/DCUV5IPC/sim.html:text/html},
}

@article{cohen_coefficient_1960,
	title = {A {Coefficient} of {Agreement} for {Nominal} {Scales}},
	volume = {20},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316446002000104},
	doi = {10.1177/001316446002000104},
	language = {en},
	number = {1},
	urldate = {2023-03-12},
	journal = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	month = apr,
	year = {1960},
	note = {Publisher: SAGE Publications Inc},
	pages = {37--46},
	file = {SAGE PDF Full Text:/home/ekr/Zotero/storage/5HIYB5TA/Cohen - 1960 - A Coefficient of Agreement for Nominal Scales.pdf:application/pdf},
}

@article{landis_measurement_1977,
	title = {The {Measurement} of {Observer} {Agreement} for {Categorical} {Data}},
	volume = {33},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2529310},
	doi = {10.2307/2529310},
	abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
	number = {1},
	urldate = {2023-03-12},
	journal = {Biometrics},
	author = {Landis, J. Richard and Koch, Gary G.},
	year = {1977},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {159--174},
	file = {JSTOR Full Text PDF:/home/ekr/Zotero/storage/YR5TJ3AJ/Landis et Koch - 1977 - The Measurement of Observer Agreement for Categori.pdf:application/pdf},
}

@article{vakayil_data_2022,
	title = {Data {Twinning}},
	volume = {15},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11574},
	doi = {10.1002/sam.11574},
	abstract = {In this work, we develop a method named Twinning for partitioning a dataset into statistically similar twin sets. Twinning is based on SPlit, a recently proposed model-independent method for optimally splitting a dataset into training and testing sets. Twinning is orders of magnitude faster than the SPlit algorithm, which makes it applicable to Big Data problems such as data compression. Twinning can also be used for generating multiple splits of a given dataset to aid divide-and-conquer procedures and k-fold cross validation.},
	language = {en},
	number = {5},
	urldate = {2023-03-12},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Vakayil, Akhil and Joseph, V. Roshan},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11574},
	keywords = {testing, training, validation, data compression, data splitting},
	pages = {598--610},
	file = {Full Text PDF:/home/ekr/Zotero/storage/DI76S66Z/Vakayil et Joseph - 2022 - Data Twinning.pdf:application/pdf;Snapshot:/home/ekr/Zotero/storage/3AUY3WSM/sam.html:text/html},
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2023-04-25},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
	pages = {179--188},
	file = {Snapshot:/home/ekr/Zotero/storage/LFS4J77K/j.1469-1809.1936.tb02137.html:text/html;Full Text PDF:/home/ekr/Zotero/storage/H5CKJQML/Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf},
}

@article{anderson_r_1996,
	title = {R. {A}. {Fisher} and {Multivariate} {Analysis}},
	volume = {11},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/2246198},
	abstract = {This paper reviews R. A. Fisher's many fundamental contributions to multivariate statistical analysis--from the derivation of the distribution of the sample correlation coefficient to discriminant analysis. The emphasis here is on the conceptual and mathematical development. All of his papers on multivariate analysis will be included in this survey.},
	number = {1},
	urldate = {2023-04-25},
	journal = {Statistical Science},
	author = {Anderson, T. W.},
	year = {1996},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {20--34},
	file = {JSTOR Full Text PDF:/home/ekr/Zotero/storage/KZW8MRZP/Anderson - 1996 - R. A. Fisher and Multivariate Analysis.pdf:application/pdf},
}

@incollection{izenman_linear_2008,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Linear {Dimensionality} {Reduction}},
	isbn = {978-0-387-78189-1},
	url = {https://doi.org/10.1007/978-0-387-78189-1_7},
	abstract = {When faced with situations involving high-dimensional data, it is natural to consider the possibility of projecting those data onto a lower-dimensional subspace without losing important information regarding some characteristic of the original variables. One way of accomplishing this reduction of dimensionality is through variable selection, also called feature selection (see Section 5.7). Another way is by creating a reduced set of linear or nonlinear transformations of the input variables. The creation of such composite variables (or features) by projection methods is often referred to as feature extraction. Usually, we wish to find those low-dimensional projections of the input data that enjoy some sort of optimality properties.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Modern {Multivariate} {Statistical} {Techniques}: {Regression}, {Classification}, and {Manifold} {Learning}},
	publisher = {Springer},
	author = {Izenman, Alan Julian},
	editor = {Izenman, Alan J.},
	year = {2008},
	doi = {10.1007/978-0-387-78189-1_7},
	pages = {195--236},
	file = {Full Text PDF:/home/ekr/Zotero/storage/FTZ5Y3C6/Izenman - 2008 - Linear Dimensionality Reduction.pdf:application/pdf},
}

@incollection{izenman_linear_2008-1,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Linear {Discriminant} {Analysis}},
	isbn = {978-0-387-78189-1},
	url = {https://doi.org/10.1007/978-0-387-78189-1_8},
	abstract = {Suppose we are given a learning set \$\${\textbackslash}mathcal\{L\}\$\$of multivariate observations (i.e., input values \$\${\textbackslash}mathfrak\{R\}{\textasciicircum}r\$\$), and suppose each observation is known to have come from one of K predefined classes having similar characteristics. These classes may be identified, for example, as species of plants, levels of credit worthiness of customers, presence or absence of a specific medical condition, different types of tumors, views on Internet censorship, or whether an e-mail message is spam or non-spam.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Modern {Multivariate} {Statistical} {Techniques}: {Regression}, {Classification}, and {Manifold} {Learning}},
	publisher = {Springer},
	author = {Izenman, Alan Julian},
	editor = {Izenman, Alan J.},
	year = {2008},
	doi = {10.1007/978-0-387-78189-1_8},
	pages = {237--280},
	file = {Full Text PDF:/home/ekr/Zotero/storage/RTEQ6CVL/Izenman - 2008 - Linear Discriminant Analysis.pdf:application/pdf},
}

@book{izenman_modern_2008,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Modern {Multivariate} {Statistical} {Techniques}},
	isbn = {978-0-387-78188-4 978-0-387-78189-1},
	url = {http://link.springer.com/10.1007/978-0-387-78189-1},
	urldate = {2023-04-23},
	publisher = {Springer},
	author = {Izenman, Alan J.},
	editor = {Casella, G. and Fienberg, S. and Olkin, I.},
	year = {2008},
	doi = {10.1007/978-0-387-78189-1},
	keywords = {Random Forest, Support Vector Machine, Boosting, bootstrap aggregating, cluster analysis, Clustering, data analysis, data mining, Factor analysis, Latent variable model, Linear discriminant analysis, machine learning, Mathematica, multidimensional scaling, Projection pursuit},
	file = {Full Text PDF:/home/ekr/Zotero/storage/JP6QV7HC/Izenman - 2008 - Modern Multivariate Statistical Techniques.pdf:application/pdf},
}

@incollection{xanthopoulos_support_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Support {Vector} {Machines}},
	isbn = {978-1-4419-9878-1},
	url = {https://doi.org/10.1007/978-1-4419-9878-1_5},
	abstract = {In this chapter we describe one of the most successful supervised learning algorithms namely suppor vector machines (SVMs). The SVM is one of the conceptually simplest algorithms whereas at the same time one of the best especially for binary classification. Here we illustrate the mathematical formulation of SVM together with its robust equivalent for the most common uncertainty sets.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Robust {Data} {Mining}},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	editor = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1_5},
	keywords = {Admissible Kernel Function, Hard Margin Classiﬁer, Optimal Separating Hyperplane, Robust Equivalent, Super Vector Machine (SVM)},
	pages = {35--48},
	file = {Full Text PDF:/home/ekr/Zotero/storage/6JQ763CD/Xanthopoulos et al. - 2013 - Support Vector Machines.pdf:application/pdf},
}

@incollection{xanthopoulos_linear_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Linear {Discriminant} {Analysis}},
	isbn = {978-1-4419-9878-1},
	url = {https://doi.org/10.1007/978-1-4419-9878-1_4},
	abstract = {In this chapter we discuss another popular data mining algorithm that can be used for supervised or unsupervised learning. Linear Discriminant Analysis (LDA) was proposed by R. Fischer in 1936. It consists in finding the projection hyperplane that minimizes the interclass variance and maximizes the distance between the projected means of the classes. Similarly to PCA, these two objectives can be solved by solving an eigenvalue problem with the corresponding eigenvector defining the hyperplane of interest. This hyperplane can be used for classification, dimensionality reduction and for interpretation of the importance of the given features. In the first part of the chapter we discuss the generic formulation of LDA whereas in the second we present the robust counterpart scheme originally proposed by Kim and Boyd. We also discuss the non linear extension of LDA through the kernel transformation.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Robust {Data} {Mining}},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	editor = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1_4},
	keywords = {Linear Discriminant Analysis, Data Mining Algorithm, Feature Space, Kernel Trick, Robust Counterpart},
	pages = {27--33},
	file = {Full Text PDF:/home/ekr/Zotero/storage/YQCTDJHK/Xanthopoulos et al. - 2013 - Linear Discriminant Analysis.pdf:application/pdf},
}

@incollection{xanthopoulos_principal_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Principal {Component} {Analysis}},
	isbn = {978-1-4419-9878-1},
	url = {https://doi.org/10.1007/978-1-4419-9878-1_3},
	abstract = {The principal component analysis (PCA) transformation is a very common and well-studied data analysis technique that aims to identify some linear trends and simple patterns in a group of samples. It has application in several areas of engineering. It is popular from computational perspective as it requires only an eigendecomposition or singular value decomposition. There are two alternative optimization approaches for obtaining principal component analysis solution, the one of variance maximization and the one of minimum error formulation. Both start with a “different” initial objective and end up providing the same solution. It is necessary to study and understand both of these alternative approaches. In the second part of this chapter we present the robust counterpart formulation of PCA and demonstrate how such a formulation can be used in practice in order to produce sparse solutions.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Robust {Data} {Mining}},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	editor = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1_3},
	keywords = {Robust Optimization, Robust Counterpart, Principal Component Analysis, Robust Principal Component Analysis, Sparse Solution},
	pages = {21--26},
	file = {Full Text PDF:/home/ekr/Zotero/storage/R4TPCTFD/Xanthopoulos et al. - 2013 - Principal Component Analysis.pdf:application/pdf},
}

@book{xanthopoulos_robust_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Robust {Data} {Mining}},
	isbn = {978-1-4419-9877-4 978-1-4419-9878-1},
	url = {https://link.springer.com/10.1007/978-1-4419-9878-1},
	language = {en},
	urldate = {2023-04-23},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1},
	keywords = {linear discriminant analysis, robust data mining, robust optimization, support vector machines},
	file = {Full Text PDF:/home/ekr/Zotero/storage/HK7EHCZ4/Xanthopoulos et al. - 2013 - Robust Data Mining.pdf:application/pdf},
}

@misc{noauthor_linear_nodate,
	title = {Linear {Discriminant} {Analysis}, {Explained} {\textbar} by {YANG} {Xiaozhou} {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b},
	urldate = {2023-04-23},
	file = {Linear Discriminant Analysis, Explained | by YANG Xiaozhou | Towards Data Science:/home/ekr/Zotero/storage/VWPXALMB/linear-discriminant-analysis-explained-f88be6c1e00b.html:text/html},
}

@book{hastie_elements_2016,
	edition = {2nd},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning} - {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	language = {en},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2016},
	keywords = {Random Forest, Support Vector Machine, Boosting, data mining, machine learning, Projection pursuit, Averaging, classification, clustering, supervised learning, unsupervised learning},
	file = {Tibshirani et Friedman - Valerie and Patrick Hastie.pdf:/home/ekr/Zotero/storage/5PN8P6WI/Tibshirani et Friedman - Valerie and Patrick Hastie.pdf:application/pdf;Full Text PDF:/home/ekr/Zotero/storage/MPN39ZYG/Hastie et al. - 2009 - The Elements of Statistical Learning.pdf:application/pdf},
}

@article{loh_fifty_2014,
	title = {Fifty {Years} of {Classification} and {Regression} {Trees}: {Fifty} {Years} of {Classification} and {Regression} {Trees}},
	volume = {82},
	issn = {03067734},
	shorttitle = {Fifty {Years} of {Classification} and {Regression} {Trees}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12016},
	doi = {10.1111/insr.12016},
	abstract = {Fifty years have passed since the publication of the ﬁrst regression tree algorithm. New techniques have added capabilities that far surpass those of the early methods. Modern classiﬁcation trees can partition the data with linear splits on subsets of variables and ﬁt nearest neighbor, kernel density, and other models in the partitions. Regression trees can ﬁt almost every kind of traditional statistical model, including least-squares, quantile, logistic, Poisson, and proportional hazards models, as well as models for longitudinal and multiresponse data. Greater availability and affordability of software (much of which is free) have played a signiﬁcant role in helping the techniques gain acceptance and popularity in the broader scientiﬁc community. This article surveys the developments and brieﬂy reviews the key ideas behind some of the major algorithms.},
	language = {en},
	number = {3},
	urldate = {2023-05-20},
	journal = {International Statistical Review},
	author = {Loh, Wei-Yin},
	month = dec,
	year = {2014},
	pages = {329--348},
	file = {Loh - 2014 - Fifty Years of Classification and Regression Trees.pdf:/home/ekr/Zotero/storage/5TEAEZ9V/Loh - 2014 - Fifty Years of Classification and Regression Trees.pdf:application/pdf},
}

@book{wu_top_2009,
	edition = {First},
	title = {The {Top} {Ten} {Algorithms} in {Data} {Mining}},
	isbn = {978-1-4200-8964-6},
	language = {en},
	publisher = {CRC Press},
	author = {Wu, Xindong and Kumar, Vipin},
	year = {2009},
	file = {_.pdf:/home/ekr/Zotero/storage/YIZIQM8Y/_.pdf:application/pdf},
}

@article{wu_top_2008,
	title = {Top 10 algorithms in data mining},
	volume = {14},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-007-0114-2},
	doi = {10.1007/s10115-007-0114-2},
	abstract = {This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association analysis, and link mining, which are all among the most important topics in data mining research and development.},
	language = {en},
	number = {1},
	urldate = {2023-05-20},
	journal = {Knowledge and Information Systems},
	author = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
	month = jan,
	year = {2008},
	keywords = {Support Vector Machine, Apriori Algorithm, Association Rule, Frequent Itemsets, Mach Learn},
	pages = {1--37},
	file = {Full Text PDF:/home/ekr/Zotero/storage/NTTEMP4E/Wu et al. - 2008 - Top 10 algorithms in data mining.pdf:application/pdf},
}

@book{rokach_data_2015,
	address = {Hackensack, New Jersey},
	edition = {Second edition},
	title = {Data mining with decision trees: theory and applications},
	isbn = {978-981-4590-07-5},
	shorttitle = {Data mining with decision trees},
	language = {en},
	publisher = {World Scientific},
	author = {Rokach, Lior and Maimon, Oded},
	year = {2015},
	keywords = {Data mining, Machine learning, Decision support systems, Decision trees},
	file = {Rokach et Maimon - 2015 - Data mining with decision trees theory and applic.pdf:/home/ekr/Zotero/storage/JVNRZ9CX/Rokach et Maimon - 2015 - Data mining with decision trees theory and applic.pdf:application/pdf},
}

@misc{noauthor_machine_nodate,
	title = {Machine {Learning} for {Biomedical} {Application} {\textbar} {MDPI} {Books}},
	url = {https://www.mdpi.com/books/book/5130},
	urldate = {2023-05-19},
	file = {Machine Learning for Biomedical Application | MDPI Books:/home/ekr/Zotero/storage/6RBABE9A/5130.html:text/html},
}

@book{zhang_recursive_2010,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Recursive {Partitioning} and {Applications}},
	volume = {0},
	isbn = {978-1-4419-6823-4 978-1-4419-6824-1},
	url = {https://link.springer.com/10.1007/978-1-4419-6824-1},
	language = {en},
	urldate = {2023-05-19},
	publisher = {Springer},
	author = {Zhang, Heping and Singer, Burton H.},
	year = {2010},
	doi = {10.1007/978-1-4419-6824-1},
	keywords = {Recursive Partitioning, classification, adIOMEDICIaptive Splines and Regression Trees, calculus, epidemiology, Logistic Regression, Practical Computational Methods, Tree-based Survival Analysis, Trees and Associated Forests},
	file = {Full Text PDF:/home/ekr/Zotero/storage/WKCG2UUD/Zhang et Singer - 2010 - Recursive Partitioning and Applications.pdf:application/pdf},
}

@book{zhang_ensemble_2012,
	address = {New York, NY},
	title = {Ensemble {Machine} {Learning}: {Methods} and {Applications}},
	isbn = {978-1-4419-9325-0 978-1-4419-9326-7},
	shorttitle = {Ensemble {Machine} {Learning}},
	url = {https://link.springer.com/10.1007/978-1-4419-9326-7},
	language = {en},
	urldate = {2023-05-19},
	publisher = {Springer},
	editor = {Zhang, Cha and Ma, Yunqian},
	year = {2012},
	doi = {10.1007/978-1-4419-9326-7},
	keywords = {machine learning, Bagging Predictors, Basic Boosting, classification algorithm, deep neural networks, Ensemble learning, Object Detection, random forest, stacked generalization, statistical classifiers},
	file = {Full Text PDF:/home/ekr/Zotero/storage/9HCD4ZUR/Zhang et Ma - 2012 - Ensemble Machine Learning Methods and Application.pdf:application/pdf},
}

@incollection{theodoridis_appendix_2015,
	address = {Oxford},
	title = {Appendix {C} - {Hints} on {Constrained} {Optimization}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099821},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09982-1},
	pages = {1023--1029},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/LNCMMQFF/Theodoridis - 2015 - Appendix C - Hints on Constrained Optimization.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/NUBVU8CQ/machine-learning.html:text/html},
}

@incollection{theodoridis_index_2015,
	address = {Oxford},
	title = {Index},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099948},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09994-8},
	pages = {1031--1050},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/DG4RWIS5/Theodoridis - 2015 - Index.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/88PTPV6Y/machine-learning.html:text/html},
}

@incollection{theodoridis_appendix_2015-1,
	address = {Oxford},
	title = {Appendix {B} - {Probability} {Theory} and {Statistics}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099833},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09983-3},
	pages = {1019--1022},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/RCVDH2PD/Theodoridis - 2015 - Appendix B - Probability Theory and Statistics.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/AS5A7YDJ/machine-learning.html:text/html},
}

@incollection{theodoridis_appendix_2015-2,
	address = {Oxford},
	title = {Appendix {A} - {Linear} {Algebra}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099845},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09984-5},
	pages = {1013--1017},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/4EJINYNP/Theodoridis - 2015 - Appendix A - Linear Algebra.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/ZL4P3P7T/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015,
	address = {Oxford},
	title = {Chapter 19 - {Dimensionality} {Reduction} and {Latent} {Variables} {Modeling}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000197},
	abstract = {This chapter deals with latent variables modeling and dimensionality reduction techniques. It starts with the more classical principle components analysis (PCA) method. Its various properties are analyzed and its interpretation as a low-rank matrix factorization is emphasized. Then, the canonical correlation analysis (CCA) and its relatives, such as partial least-squares (PLS) are introduced. Independent component analysis (ICA) is reviewed and the cocktail party problem is presented. Dictionary learning, as a matrix factorization approach, is defined and the k-SVD algorithm is considered. The probabilistic approach to latent variables modeling is reviewed, starting with the factor analysis method and moves on to discuss probabilistic PCA and the method of mixture of factor analyzers, as a Bayesian alternative to compressed sensing. Nonlinear dimensionality reduction techniques, including kernel PCA, Laplacian eigenmaps, local linear embedding (LLE), and isometric mapping (ISOPAM) are considered. Matrix completion and robust PCA are introduced and related applications are discussed. Finally, the chapter concludes with a case study concerning fMRI data-analysis.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00019-7},
	keywords = {Factor analysis, -SVD algorithm, Canonical correlation analysis (CCA), Dictionary learning, fMRI, Independent component analysis (ICA), Isometric mapping (ISOMAP), Kernel PCA, Laplacian eigenmaps, Local Linear Embedding (LLE), Low-rank matrix factorization, Matrix completion, Mixture of factor analyzers, Nonnegative matrix factorization, Partial least-squares (PLS), Principle component analysis (PCA), Principle component pursuit (PCP), Probabilistic PCA, Robust PCA},
	pages = {937--1011},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/YUEMRZVG/Theodoridis - 2015 - Chapter 19 - Dimensionality Reduction and Latent V.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/VZCEBMD3/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-1,
	address = {Oxford},
	title = {Chapter 16 - {Probabilistic} {Graphical} {Models}: {Part} {II}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 16 - {Probabilistic} {Graphical} {Models}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000173},
	abstract = {This chapter is the second one dealing with probabilistic graphical models. Junction trees are first reviewed and a message-passing algorithm for such structures is developed. Then, the focus turns on approximate inference techniques on graphical models, based on variational methods, both for local as well as global approximation. Dynamic graphical models are discussed with an emphasis on HMMs. Inference and training of HMMs is viewed as a special case of the EM algorithm and the message-passing rationale. The Baum-Welch and the Viterbi algorithms are derived. Finally, some extensions, including factorial HMMs and time-varying dynamic Bayesian networks are presented. A discussion on parameters learning in graphical models is provided at the end of the chapter.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00017-3},
	keywords = {Approximate inference, Boltzmann machines, Factorial HMMs, HMMs, Junction trees, Loopy belief propagation, Message-passing, Multiple-cause networks, Perfect elimination sequence, Speech recognition, Variational methods},
	pages = {795--843},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/LEK25X5W/Theodoridis - 2015 - Chapter 16 - Probabilistic Graphical Models Part .pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/YWPQWPK6/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-2,
	address = {Oxford},
	title = {Chapter 18 - {Neural} {Networks} and {Deep} {Learning}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000185},
	abstract = {This chapter deals with neural networks (NN), starting from the early days of the perceptron and perceptron rule, then moves on to review multilayer feed-forward neural networks and the backpropagation algorithm. The drawbacks of training NN with many layers, via the backpropagation algorithm, are discussed together with the advantages that one would expect to obtain if such networks could be trained efficiently. Restricted Boltzmann machines (RBM) are then discussed and the contrastive divergence algorithm is presented as the vehicle to pre-train deep/many layer NN architectures. Deep belief networks, conditional RBMs and autoencoders are also discussed. Finally, two case studies are presented, concerning the use of deep NN architectures, one in the context of OCR and another in the context of autoencoding.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00018-5},
	keywords = {Autoencoders, Conditional restricted Boltzmann machines, Contrastive divergence, Deep belief networks, Kernel perceptron algorithm, Multilayer feed-forward neural networks, Optical Character Recognition (OCR), Perceptron and perceptron rule, Restricted Boltzmann machines, Universal approximation properties},
	pages = {875--936},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/QB8YQ4XP/Theodoridis - 2015 - Chapter 18 - Neural Networks and Deep Learning.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/U8CBBA75/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-3,
	address = {Oxford},
	title = {Chapter 17 - {Particle} {Filtering}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B978012801522300015X},
	abstract = {In this chapter, sequential sampling techniques are considered. Kalman filtering is viewed in terms of probabilistic arguments as a special case of a linear dynamic system, where the involved variables follow Gaussian distributions. Particle filtering techniques are then considered as a vehicle to treat more general nonlinear models and/or non-Gaussian random variables. They are introduced as a special instance of the more general family of sequential sampling methods. Different schemes are discussed such as the generic particle and the auxiliary particle filtering algorithms.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00015-X},
	keywords = {Auxiliary particle filtering, Degeneracy, Generic particle filters, Kalman filtering, Particle filtering, Resampling, Sequential importance sampling, Stochastic volatility models, Visual tracking},
	pages = {845--873},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/6ES7WC4V/Theodoridis - 2015 - Chapter 17 - Particle Filtering.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/YN2LRF6P/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-4,
	address = {Oxford},
	title = {Chapter 15 - {Probabilistic} {Graphical} {Models}: {Part} {I}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 15 - {Probabilistic} {Graphical} {Models}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000161},
	abstract = {In many everyday machine learning applications involving multivariate statistical modeling, even simple inference tasks can easily become computationally intractable. Graph theory has proved a powerful and elegant tool that has extensively been used in optimization and computational theory. This chapter is the first of two chapters dedicated to probabilistic graphical models. Bayesian networks are introduced and the Markov condition is discussed. The d-separation property of Bayesian networks is reviewed. Markov random fields and conditional Markov fields are presented. Factor graphs are defined and emphasis is given in developing message-passing algorithms for chains and trees.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00016-1},
	keywords = {-separation, Bayesian networks, Conditional random fields, Factor graphs, Ising and Potts models, Markov condition, Markov random fields, Message-passing algorithms for chains and trees, Probabilistic graphical models},
	pages = {745--793},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/WI99RCIB/Theodoridis - 2015 - Chapter 15 - Probabilistic Graphical Models Part .pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/NRSLXDY6/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-5,
	address = {Oxford},
	title = {Chapter 14 - {Monte} {Carlo} {Methods}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000148},
	abstract = {This chapter deals with basic concepts and definitions concerning Monte Carlo sampling techniques. Rejection sampling and importance sampling are first introduced. Markov chains and some of their basic properties are discussed. Then the Metropolis-Hastings and Gibbs algorithms are presented. At the end of the chapter, a case study concerning change point detection is considered.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00014-8},
	keywords = {Change point detection, Gibbs sampling, Importance sampling, Markov chains, Metropolis-Hastings algorithm, Monte Carlo methods, Rejection sampling},
	pages = {707--744},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/QPK2C6YZ/Theodoridis - 2015 - Chapter 14 - Monte Carlo Methods.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/WTBHA6NI/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-6,
	address = {Oxford},
	title = {Chapter 13 - {Bayesian} {Learning}: {Approximate} {Inference} and {Nonparametric} {Models}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 13 - {Bayesian} {Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000136},
	abstract = {This chapter is the second one dedicated to Bayesian learning. The emphasis here is on more advanced topics, dealing with approximate inference methods. Two paths for approximate inference, known as variational techniques, are discussed. One is based on the mean field approximation and the lower bound interpretation of the EM, and the other on convex duality and variational bounds. Regression and mixture modeling are discussed in this framework. Emphasis is given to sparse Bayesian modeling techniques and hierarchical Bayesian models. The relevance vector machine framework is presented. Expectation propagation is also discussed as an alternative to variational methods for approximate inference. At the end of the chapter, Bayesian learning in the context of nonparametric models is discussed, including Gaussian processes. Finally, a case study concerning hyperspectral imaging is presented.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00013-6},
	keywords = {Bayesian learning, Chinese restaurant process, Dirichlet processes, Expectation propagation, Exponential family, Gaussian mixture modeling, Gaussian processes, Hyperspectral image unmixing, Linear regression, Nonparametric Bayesian modeling, Relevance vector machines, Sparse Bayesian learning, Stick breaking construction, Variational approximation},
	pages = {639--706},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/CE3G3QQ7/Theodoridis - 2015 - Chapter 13 - Bayesian Learning Approximate Infere.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/PURMMW8C/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-7,
	address = {Oxford},
	title = {Chapter 12 - {Bayesian} {Learning}: {Inference} and the {EM} {Algorithm}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 12 - {Bayesian} {Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000124},
	abstract = {This is the first of two chapters dedicated to Bayesian learning. The main concepts and philosophy behind Bayesian inference are introduced. The evidence function and its relation to Occam’s razor rule are presented. The expectation-maximization (EM) algorithm is derived and applied to linear regression and Gaussian mixture modeling. The k-means algorithm for clustering and its affinity to Gaussian mixture modeling are discussed. Finally, the concept of probabilistic model mixing is reviewed and the notion of mixture of experts is presented.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00012-4},
	keywords = {-means algorithm, Bayesian method, Evidence function, Expectation-maximization (EM) algorithm, Laplacian approximation method, Maximum a posteriori probability (MAP) estimator, Maximum likelihood estimator, Mixture of experts},
	pages = {585--638},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/ST22AEG8/Theodoridis - 2015 - Chapter 12 - Bayesian Learning Inference and the .pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/FNUKNSUP/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-8,
	address = {Oxford},
	title = {Chapter 11 - {Learning} in {Reproducing} {Kernel} {Hilbert} {Spaces}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000112},
	abstract = {This chapter is dedicated to nonparametric modeling of nonlinear functions in reproducing kernel Hilbert spaces (RKHS). The basic definitions and concepts behind RKH spaces are presented, including positive definite kernels, reproducing kernels, kernel matrices, and the kernel trick. Cover’s theorem and the representer theorem are introduced. Then, kernel ridge regression, support vector regression, and support vector machines are studied. Online algorithms for learning in RKH spaces, such as the kernel LMS, NORMA, and the kernel APSM are discussed. The notion of multiple kernel learning is presented and a discussion on sparse modeling for nonparametric models in the context of additive models is provided. The chapter closes with a case study for text authorship identification via the use of string kernels.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00011-2},
	keywords = {Additive models, Authorship identification., Cover’s theorem, KAPSM, Kernel ridge regression, KLMS, Multiple kernels, NORMA, Positive definite kernels, Representer theorem, Reproducing kernels, RKHS, Support vector machines, Support vector regression},
	pages = {509--583},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/EFHRMBGG/Theodoridis - 2015 - Chapter 11 - Learning in Reproducing Kernel Hilber.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/G82WH8IC/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-9,
	address = {Oxford},
	title = {Chapter 10 - {Sparsity}-{Aware} {Learning}: {Algorithms} and {Applications}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 10 - {Sparsity}-{Aware} {Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000100},
	abstract = {This chapter is the follow-up to the previous one concerning sparsity-aware learning. The emphasis now is on the algorithmic front. Greedy, iterative thresholding and convex optimization algorithms are presented and discussed, both for batch as well as online learning. Extensions of the ℓ1 norm regularization are introduced, such as group sparse modeling, structured sparsity, total variation. The issue of analysis versus synthesis sparse modeling is presented together with the notion of co-sparsity. Finally, a case study of sparse modeling for time-frequency analysis in the context of Gabor frames is demonstrated.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00010-0},
	keywords = {Adaptive LASSO, AdCoSaMP, Cosparsity, CSMP, Gabor frames., Group sparsity, ITS, LARS-LASSO, Message passing, NESTA, OMP, Phase transition diagram, SpAPSM, SpaRSA, Sparsity-promoting algorithms, Structured sparsity, Time-frequency analysis, Total variation},
	pages = {449--507},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/YHV5ILMY/Theodoridis - 2015 - Chapter 10 - Sparsity-Aware Learning Algorithms a.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/AZAUHDEV/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-10,
	address = {Oxford},
	title = {Chapter 9 - {Sparsity}-{Aware} {Learning}: {Concepts} and {Theoretical} {Foundations}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 9 - {Sparsity}-{Aware} {Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000094},
	abstract = {This chapter presents the main concepts and theoretical foundations related to sparsity-aware learning techniques. The concept of sparse modeling is introduced together with the LASSO and the ℓ0 and ℓ1 norm minimizing tasks. Conditions for uniqueness of the obtained solutions as well as for the equivalence of the ℓ0 and ℓ1 norm minimization are stated. The RIP condition and related bounds are discussed. Compressed sensing and the notion of stable embeddings are reviewed. The concept of sub-Nyquist sampling is presented and finally a case study concerning image de-nosing is reported.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00009-4},
	keywords = {and  norms, Basis pursuit, Compressed sensing, De-noising, LASSO, Mutual coherence, RIP, Spark, Sparse modeling, Sub-Nyquist sampling},
	pages = {403--448},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/YJ9FBYBB/Theodoridis - 2015 - Chapter 9 - Sparsity-Aware Learning Concepts and .pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/XJN4RIP3/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-11,
	address = {Oxford},
	title = {Chapter 6 - {The} {Least}-{Squares} {Family}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000069},
	abstract = {In Chapter 6, the sum of least-squares cost function is reconsidered. The LS estimator is rederived via geometric arguments and its properties are discussed. The Ridge regression formulation is viewed via geometric arguments. The SVD matrix factorization method is presented and the concept of low rank matrix approximation is introduced. Emphasis is given on the RLS algorithm for the iterative solution of the LS cost function and its relation to Newton’s optimization method is established. The Coordinate descent scheme for iterative optimization is defined as an alternative to the steepest descent and Newton’s approaches. Finally, the method of total-least-squares is reviewed.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00006-9},
	keywords = {Coordinate descent method, Least-squares estimator, Newton’s method, Pseudo-inverse matrix, Recursive least-squares algorithm, Ridge regression, Singular value decomposition, Total-least-squares method},
	pages = {233--274},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/X26YICKD/Theodoridis - 2015 - Chapter 6 - The Least-Squares Family.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/WGKUWS6X/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-12,
	address = {Oxford},
	title = {Chapter 8 - {Parameter} {Learning}: {A} {Convex} {Analytic} {Path}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 8 - {Parameter} {Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000082},
	abstract = {The goal of this chapter is to present an overview of techniques for convex optimization in the context of machine learning. It starts from the definitions of convex sets, functions and the projection operator and some of its properties are derived. The fundamental theorem of POCS and its more recent online version, APSM, are presented. Then, the topic of minimizing nonsmooth convex functions is discussed and the definitions of subgradient and subdifferential are provided. The method of subgradient iterative minimization and some of its versions are presented. The regret analysis technique is discussed. The chapter closes with presenting the proximal approximation, ADMM and the mirror descent approaches for optimization.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00008-2},
	keywords = {ADMM, APSM, Convex sets and functions, Mirror descent algorithms, POCS, Projection operator, Proximal minimization, Proximal operator, Regret analysis, Subgradient and Subdifferential, Subgradient iterative optimization},
	pages = {327--402},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/3ARQZ7YX/Theodoridis - 2015 - Chapter 8 - Parameter Learning A Convex Analytic .pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/3P3Z4IHJ/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-13,
	address = {Oxford},
	title = {Chapter 7 - {Classification}: {A} {Tour} of the {Classics}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 7 - {Classification}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000070},
	abstract = {The goal of this chapter is to present some of the more classical techniques for classification. These methods is a must for any newcomer in the field. Bayesian decision theory is first reviewed and the concepts of discriminant functions and decision surfaces are introduced. Then, minimum distance classifiers are presented as a special instance of the Bayesian classification. The naive Bayes classifier is discussed and the design of linear models for classification are presented, including logistic regression and Fisher’s linear discriminant method. Then, decision trees are introduced. The technique of combining classifiers is discussed, and the Adaboost algorithm and the method of boosted trees is reviewed. Finally, a case study for protein folding prediction is presented.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00007-0},
	keywords = {Decision trees, AdaBoost, Average risk, Bayesian classification, Boosted decision trees, Classification, Combining classifiers, Logistic regression, Minimum distance classifiers, Naive Bayes classifier, Protein folding prediction},
	pages = {275--325},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/BBLIJVH4/Theodoridis - 2015 - Chapter 7 - Classification A Tour of the Classics.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/DMJBP77A/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-14,
	address = {Oxford},
	title = {Chapter 5 - {Stochastic} {Gradient} {Descent}: {The} {LMS} {Algorithm} and its {Family}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 5 - {Stochastic} {Gradient} {Descent}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000057},
	abstract = {The focus of this chapter is to introduce the stochastic gradient descent family of online/adaptive algorithms in the framework of the squared error loss function. The gradient descent approach to optimization is presented and the stochastic approximation method is discussed. Then, the LMS algorithm and its offsprings, such as the APA and the NLMS are introduced. Finally, distributed learning is discussed with an emphasis to distributed versions of the LMS.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00005-7},
	keywords = {Affine projection algorithm, Diffusion LMS, Distributed learning, Gradient descent method, Least-mean-squares LMS adaptive algorithm, Method of stochastic approximation, Robbins-Monro algorithm, Steepest descent method},
	pages = {161--231},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/JWHH53PW/Theodoridis - 2015 - Chapter 5 - Stochastic Gradient Descent The LMS A.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/H36YDVHD/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-15,
	address = {Oxford},
	title = {Chapter 4 - {Mean}-{Square} {Error} {Linear} {Estimation}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000045},
	abstract = {In this chapter, mean-square linear estimation is discussed and the normal equations are derived. The orthogonality theorem concerning random variables is introduced as an alternative for their derivation. Issues concerning complex random variables, such as widely-linear estimation and Wirtinger’s calculus are presented. Some typical applications of MSE, such as image deblurring, interference cancellation, system identification and channel equalization are defined. Issues related to the efficient solution of the normal equations in the context of linear filtering are discussed and the Levinson and lattice-ladder algorithms are derived. The Gauss-Markov theorem for MSE of linear models is provided and the case of constrained estimation in the context of beamforming is introduced. The chapter concludes with the introduction and derivation of Kalman filtering.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00004-5},
	keywords = {Kalman filtering, Beamforming, Complex-valued variables, Deconvolution, Equalization, Gauss-Markov theorem, Interference cancellation, Levinson’s algorithm, Linear filtering, MSE estimation, Orthogonality condition, Wirtinger calculus},
	pages = {105--160},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/WXKEC4XC/Theodoridis - 2015 - Chapter 4 - Mean-Square Error Linear Estimation.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/EH3G46D6/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-16,
	address = {Oxford},
	title = {Chapter 3 - {Learning} in {Parametric} {Modeling}: {Basic} {Concepts} and {Directions}},
	isbn = {978-0-12-801522-3},
	shorttitle = {Chapter 3 - {Learning} in {Parametric} {Modeling}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000033},
	abstract = {The chapter presents an overview of basic directions in machine learning and the basic notions related to parametric modeling are introduced. The tasks of regression and classification are defined and basic concepts related to parameter estimation are outlined such as estimator efficiency, Cramér-Rao bound, sufficient statistic. The least-squares estimator and some of its properties are discussed. The notions of inverse problems, overfitting, bias-variance dilemma and regularization are presented. The methods of maximum likelihood, maximum a posteriori and Bayesian inference are introduced. Finally, the curse of dimensionality and the cross-validation technique are provided. The chapter closes with a discussion on nonparametric models with an emphasis on Parzen windows and the k-nearest neighbor density estimation approach.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00003-3},
	keywords = {Validation, Linear regression, Classification, Bayesian methods, Bias-variance dilemma, Least-squares (LS) estimator, Maximum likelihood (ML) method, Mean-square error (MSE), Nonparametric modeling, Regularization},
	pages = {53--103},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/EXQT2NLS/Theodoridis - 2015 - Chapter 3 - Learning in Parametric Modeling Basic.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/IIFNG7ZM/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-17,
	address = {Oxford},
	title = {Chapter 2 - {Probability} and {Stochastic} {Processes}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223000021},
	abstract = {This chapter is an introduction to probability and statistics, providing basic definitions and properties related to probability theory and stochastic processes to help readers refresh their memory and to establish a common language and a commonly understood notation. Besides probability and random variables, random processes are briefly reviewed, and some basic theorems are stated. Readers who are interested in statistical signal processing/adaptive processing can focus on the random processes section, while those who ascribe to a probabilistic machine learning point of view can focus on the section presenting various distributions. At the end of the chapter, basic definitions and properties related to information theory are summarized.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00002-1},
	keywords = {Ergodicity, Gaussian, Information theory, Mean value, Power spectral density, Probability, Random variable, Stationarity, Stochastic process, Variance},
	pages = {9--51},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/855RUFN6/Theodoridis - 2015 - Chapter 2 - Probability and Stochastic Processes.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/MF9SMYC6/machine-learning.html:text/html},
}

@incollection{theodoridis_chapter_2015-18,
	address = {Oxford},
	title = {Chapter 1 - {Introduction}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B978012801522300001X},
	abstract = {This chapter serves as an introduction to the text and an overview of machine learning. It deals with two problems at the heart of machine learning and of the book—classification and regression tasks. The chapter also outlines the structure of the book and provides a road map for students and instructors. A summary of each chapter is provided. The first six chapters of the book deal with classical topics, while the remaining twelve cover more advanced techniques. Finally, the author offers suggestions on which chapters to cover based on the focus of the particular course.},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	author = {Theodoridis, Sergios},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00001-X},
	keywords = {Machine learning, Bayesian learning, Classification, Adaptive signal processing, Regression, Statistical signal processing},
	pages = {1--8},
	file = {ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/XA8DMPG6/Theodoridis - 2015 - Chapter 1 - Introduction.pdf:application/pdf;ScienceDirect Snapshot:/home/ekr/Zotero/storage/GVTRRSWG/machine-learning.html:text/html},
}

@incollection{theodoridis_notation_2015,
	address = {Oxford},
	title = {Notation},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099882},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09988-2},
	pages = {xxi},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/ZG4IMBBZ/machine-learning.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/YWJW2ZVM/Theodoridis - 2015 - Notation.pdf:application/pdf},
}

@incollection{theodoridis_dedication_2015,
	address = {Oxford},
	title = {Dedication},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099869},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09986-9},
	pages = {xxiii},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/WS7M9KLR/machine-learning.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/VML5YYSI/Theodoridis - 2015 - Dedication.pdf:application/pdf},
}

@incollection{theodoridis_copyright_2015,
	address = {Oxford},
	title = {Copyright},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B978012801522309995X},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09995-X},
	pages = {iv},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/4Q6SBXAN/machine-learning.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/E4WTVD9K/Theodoridis - 2015 - Copyright.pdf:application/pdf},
}

@incollection{theodoridis_front_2015,
	address = {Oxford},
	title = {Front {Matter}},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099936},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09993-6},
	pages = {i--ii},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/KSXB8CAJ/machine-learning.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/43D2AN43/Theodoridis - 2015 - Front Matter.pdf:application/pdf},
}

@incollection{theodoridis_acknowledgments_2015,
	address = {Oxford},
	title = {Acknowledgments},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099894},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09989-4},
	pages = {xix},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/DBC38VTT/machine-learning.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/3FA9MM3Q/Theodoridis - 2015 - Acknowledgments.pdf:application/pdf},
}

@incollection{theodoridis_preface_2015,
	address = {Oxford},
	title = {Preface},
	isbn = {978-0-12-801522-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128015223099870},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Academic Press},
	editor = {Theodoridis, Sergios},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.09987-0},
	pages = {xvii},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/4ZTT4VKR/machine-learning.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/5PQHWRMU/Theodoridis - 2015 - Preface.pdf:application/pdf},
}

@incollection{theodoridis_classification_2015,
	title = {Classification},
	isbn = {978-0-12-801522-3},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128015223000070},
	language = {en},
	urldate = {2023-05-19},
	booktitle = {Machine {Learning}},
	publisher = {Elsevier},
	author = {Theodoridis, Sergios},
	year = {2015},
	doi = {10.1016/B978-0-12-801522-3.00007-0},
	pages = {275--325},
	file = {Theodoridis - 2015 - Classification.pdf:/home/ekr/Zotero/storage/5Q787778/Theodoridis - 2015 - Classification.pdf:application/pdf},
}

@book{lytras_artificial_2020,
	title = {Artificial {Intelligence} for {Smart} and {Sustainable} {Energy} {Systems} and {Applications}},
	url = {https://www.mdpi.com/books/book/2319},
	abstract = {Energy has been a crucial element for human beings and sustainable development. The issues of global warming and non-green energy have yet to be resolved. This book is a collection of twelve articles that provide strong evidence for the success of artificial intelligence deployment in energy research, particularly research devoted to non-intrusive load monitoring, network, and grid, as well as other emerging topics. The presented artificial intelligence algorithms may provide insight into how to apply similar approaches, subject to fine-tuning and customization, to other unexplored energy research. The ultimate goal is to fully apply artificial intelligence to the energy sector. This book may serve as a guide for professionals, researchers, and data scientists\&mdash;namely, how to share opinions and exchange ideas so as to facilitate a better fusion of energy, academic, and industry research, and improve in the quality of people's daily life activities.},
	language = {English},
	urldate = {2023-05-19},
	publisher = {MDPI - Multidisciplinary Digital Publishing Institute},
	editor = {Lytras, Miltiadis and Chui, Kwok Tai},
	month = may,
	year = {2020},
	doi = {10.3390/books978-3-03928-890-8},
	file = {Full Text PDF:/home/ekr/Zotero/storage/HIRM8WLA/Lytras et Chui - 2020 - Artificial Intelligence for Smart and Sustainable .pdf:application/pdf},
}

@book{noauthor_resampling_2006,
	address = {Boston, MA},
	title = {Resampling {Methods}},
	isbn = {978-0-8176-4386-7},
	url = {http://link.springer.com/10.1007/0-8176-4444-X},
	language = {en},
	urldate = {2023-05-19},
	publisher = {Birkhäuser},
	year = {2006},
	doi = {10.1007/0-8176-4444-X},
	keywords = {data analysis, classification, Resampling, Analysis, Excel, MATLAB, model, modeling, Permutation, programming, SAS, Stata},
	file = {Full Text PDF:/home/ekr/Zotero/storage/DYHM7GHV/2006 - Resampling Methods.pdf:application/pdf},
}

@book{lantz_machine_2015,
	title = {Machine {Learning} with {R} - {Second} {Edition} {Ed}. 2},
	isbn = {978-1-78439-390-8},
	url = {https://univ.scholarvox.com/book/88853156},
	abstract = {Updated and upgraded to the latest libraries and most modern thinking, Machine Learning with R, Second Edition provides you with a rigorous introduction to this essential skill of professional data science. Without shying away from technical theory, it is written to provide focused and practical knowledge to get you building algorithms and crunching your data, with minimal previous experience.With this book, you'll discover all the analytical tools you need to gain insights from complex data and learn how to choose the correct algorithm for your specific needs. Through full engagement with the sort of real-world problems data-wranglers face, you'll learn to apply machine learning methods to deal with common tasks, including classification, prediction, forecasting, market analysis, and clustering.},
	urldate = {2023-05-19},
	publisher = {Packt Publishing},
	author = {Lantz, Brett},
	year = {2015},
	file = {Snapshot:/home/ekr/Zotero/storage/2CPDDLNN/88853156.html:text/html},
}

@book{moshkov_comparative_2020,
	address = {Cham},
	series = {Intelligent {Systems} {Reference} {Library}},
	title = {Comparative {Analysis} of {Deterministic} and {Nondeterministic} {Decision} {Trees}},
	volume = {179},
	isbn = {978-3-030-41727-7 978-3-030-41728-4},
	url = {http://link.springer.com/10.1007/978-3-030-41728-4},
	language = {en},
	urldate = {2023-05-19},
	publisher = {Springer International Publishing},
	author = {Moshkov, Mikhail},
	year = {2020},
	doi = {10.1007/978-3-030-41728-4},
	keywords = {Decision Trees, Deterministic Decision Trees, Global Approach, Local Approach, Local Decision Trees},
	file = {Full Text PDF:/home/ekr/Zotero/storage/4S6TDMG4/Moshkov - 2020 - Comparative Analysis of Deterministic and Nondeter.pdf:application/pdf},
}

@book{kretowski_evolutionary_2019,
	address = {Cham},
	series = {Studies in {Big} {Data}},
	title = {Evolutionary {Decision} {Trees} in {Large}-{Scale} {Data} {Mining}},
	volume = {59},
	isbn = {978-3-030-21850-8 978-3-030-21851-5},
	url = {http://link.springer.com/10.1007/978-3-030-21851-5},
	language = {en},
	urldate = {2023-05-19},
	publisher = {Springer International Publishing},
	author = {Kretowski, Marek},
	year = {2019},
	doi = {10.1007/978-3-030-21851-5},
	keywords = {Decision Trees, Distributed Computing, Evolutionary Computation, Evolutionary Decision Trees, Evolutionary Induction of Decision Trees, Large-Scale Data Mining},
	file = {Full Text PDF:/home/ekr/Zotero/storage/5VP39VK8/Kretowski - 2019 - Evolutionary Decision Trees in Large-Scale Data Mi.pdf:application/pdf},
}

@misc{chandrajit_geometric_2020,
	title = {Geometric {Foundations} of {Data} {Sciences} : {LDA} \& {KDA}},
	language = {en},
	publisher = {University of Texas},
	author = {Chandrajit, Bajaj},
	year = {2020},
	file = {Spring_2020_Lecture_Note_LDA-KDA.pdf:/home/ekr/Zotero/storage/M677GLNP/Spring_2020_Lecture_Note_LDA-KDA.pdf:application/pdf},
}

@book{bouveyron_model-based_2019,
	edition = {1},
	title = {Model-{Based} {Clustering} and {Classification} for {Data} {Science}: {With} {Applications} in {R}},
	isbn = {978-1-108-64418-1 978-1-108-49420-5},
	shorttitle = {Model-{Based} {Clustering} and {Classification} for {Data} {Science}},
	url = {https://www.cambridge.org/core/product/identifier/9781108644181/type/book},
	language = {en},
	urldate = {2023-05-19},
	publisher = {Cambridge University Press},
	author = {Bouveyron, Charles and Celeux, Gilles and Murphy, T. Brendan and Raftery, Adrian E.},
	month = jul,
	year = {2019},
	doi = {10.1017/9781108644181},
	file = {Bouveyron et al. - 2019 - Model-Based Clustering and Classification for Data.pdf:/home/ekr/Zotero/storage/CQULW5Z3/Bouveyron et al. - 2019 - Model-Based Clustering and Classification for Data.pdf:application/pdf},
}

@book{genuer_random_2020,
	address = {Cham},
	series = {Use {R}!},
	title = {Random {Forests} with {R}},
	isbn = {978-3-030-56484-1 978-3-030-56485-8},
	url = {http://link.springer.com/10.1007/978-3-030-56485-8},
	language = {en},
	urldate = {2023-05-19},
	publisher = {Springer International Publishing},
	author = {Genuer, Robin and Poggi, Jean-Michel},
	year = {2020},
	doi = {10.1007/978-3-030-56485-8},
	file = {Genuer et Poggi - 2020 - Random Forests with R.pdf:/home/ekr/Zotero/storage/YACBZ4F7/Genuer et Poggi - 2020 - Random Forests with R.pdf:application/pdf},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2023-06-16},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {classification, ensemble, regression},
	pages = {5--32},
	file = {Full Text PDF:/home/ekr/Zotero/storage/ZBR7VIW8/Breiman - 2001 - Random Forests.pdf:application/pdf},
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00058655},
	doi = {10.1007/BF00058655},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	language = {en},
	number = {2},
	urldate = {2023-06-16},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	keywords = {Averaging, Aggregation, Bootstrap, Combining},
	pages = {123--140},
	file = {Full Text PDF:/home/ekr/Zotero/storage/H45LVNSQ/Breiman - 1996 - Bagging predictors.pdf:application/pdf},
}

@book{michael_machine_2019,
	address = {Birmingham},
	title = {Machine {Learning} with {Go} {Quick} {Start} {Guide} : {Hands}-on techniques for building supervised and unsupervised machine learning workﬂows},
	isbn = {978-1-83855-165-0},
	language = {eng},
	publisher = {Packt Publishing},
	author = {Michael, Bironneau and Toby, Coleman and Bironneau, Michael},
	year = {2019},
	note = {Publication Title: Machine Learning with Go Quick Start Guide : Hands-on techniques for building supervised and unsupervised machine learning workﬂows},
}

@book{amr_hands-machine_2020,
	address = {Birmingham},
	title = {Hands-{On} {Machine} {Learning} with scikit-learn and {Scientific} {Python} {Toolkits}: {A} practical guide to implementing supervised and unsupervised machine learning algorithms in {Python}},
	isbn = {1-83882-358-1},
	abstract = {Integrate scikit-learn with various tools such as NumPy, pandas, imbalanced- learn, and scikit-surprise and use it to solve real-world machine learning problems Key Features * Delve into machine learning with this comprehensive guide to scikit-learn and scientific Python * Master the art of data-driven problem-solving with hands-on examples * Foster your theoretical and practical knowledge of supervised and unsupervised machine learning algorithms Book Description Machine learning is applied everywhere, from business to research and academia, while scikit-learn is a versatile library that is popular among machine learning practitioners. This book serves as a practical guide for anyone looking to provide hands-on machine learning solutions with scikit- learn and Python toolkits. The book begins with an explanation of machine learning concepts and fundamentals, and strikes a balance between theoretical concepts and their applications. Each chapter covers a different set of algorithms, and shows you how to use them to solve real-life problems. You'll also learn about various key supervised and unsupervised machine learning algorithms using practical examples. Whether it is an instance-based learning algorithm, Bayesian estimation, a deep neural network, a tree-based ensemble, or a recommendation system, you'll gain a thorough understanding of its theory and learn when to apply it. As you advance, you'll learn how to deal with unlabeled data and when to use different clustering and anomaly detection algorithms. By the end of this machine learning book, you'll have learned how to take a data-driven approach to provide end-to-end machine learning solutions. You'll also have discovered how to formulate the problem at hand, prepare required data, and evaluate and deploy models in production. What you will learn * Understand when to use supervised, unsupervised, or reinforcement learning algorithms * Find out how to collect and prepare your data for machine learning tasks * Tackle imbalanced data and optimize your algorithm for a bias or variance tradeoff * Apply supervised and unsupervised algorithms to overcome various machine learning challenges * Employ best practices for tuning your algorithm's hyper parameters * Discover how to use neural networks for classification and regression * Build, evaluate, and deploy your machine learning solutions to production Who this book is for This book is for data scientists, machine learning practitioners, and anyone who wants to learn how machine learning algorithms work and to build different machine learning models using the Python ecosystem. The book will help you take your knowledge of machine learning to the next level by grasping its ins and outs and tailoring it to your needs. Working knowledge of Python and a basic understanding of underlying mathematical and statistical concepts is required.},
	language = {eng},
	publisher = {Packt Publishing},
	author = {Amr, Tarek},
	year = {2020},
	note = {Publication Title: Hands-On Machine Learning with scikit-learn and Scientific Python Toolkits},
	keywords = {Machine learning, Python (Computer program language)},
}
