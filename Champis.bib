
@inproceedings{guyon_scaling_1997,
	title = {A {Scaling} {Law} for the {Validation}-{Set} {Training}-{Set} {Size} {Ratio}},
	url = {https://www.semanticscholar.org/paper/A-Scaling-Law-for-the-Validation-Set-Training-Set-Subramanian/452e6c05d46e061290fefff8b46d0ff161998677},
	abstract = {We address the problem of determining what fraction of the training set should be reserved as development test set or validation set. We determine that the ratio of the validation set size over the training set size scales like the square root of two complexity parameters: the complexity of the second level of inference (minimizing the validation error) over the complexity of the rst level of inference (minimizing the error rate on the training set).},
	urldate = {2023-02-15},
	author = {Guyon, Isabelle},
	year = {1997},
	file = {Subramanian - 1997 - A Scaling Law for the Validation-Set Training-Set .pdf:/home/ekr/Zotero/storage/JIBUUSH5/Subramanian - 1997 - A Scaling Law for the Validation-Set Training-Set .pdf:application/pdf},
}

@article{mak_support_2018,
	title = {Support points},
	volume = {46},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Support-points/10.1214/17-AOS1629.full},
	doi = {10.1214/17-AOS1629},
	abstract = {This paper introduces a new way to compact a continuous probability distribution \$F\$ into a set of representative points called support points. These points are obtained by minimizing the energy distance, a statistical potential measure initially proposed by Székely and Rizzo [InterStat 5 (2004) 1–6] for testing goodness-of-fit. The energy distance has two appealing features. First, its distance-based structure allows us to exploit the duality between powers of the Euclidean distance and its Fourier transform for theoretical analysis. Using this duality, we show that support points converge in distribution to \$F\$, and enjoy an improved error rate to Monte Carlo for integrating a large class of functions. Second, the minimization of the energy distance can be formulated as a difference-of-convex program, which we manipulate using two algorithms to efficiently generate representative point sets. In simulation studies, support points provide improved integration performance to both Monte Carlo and a specific quasi-Monte Carlo method. Two important applications of support points are then highlighted: (a) as a way to quantify the propagation of uncertainty in expensive simulations and (b) as a method to optimally compact Markov chain Monte Carlo (MCMC) samples in Bayesian computation.},
	number = {6A},
	urldate = {2023-02-15},
	journal = {The Annals of Statistics},
	author = {Mak, Simon and Joseph, V. Roshan},
	month = dec,
	year = {2018},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62E17, Bayesian computation, energy distance, Monte Carlo, numerical integration, quasi-Monte Carlo, representative points},
	pages = {2562--2592},
	file = {Full Text PDF:/home/ekr/Zotero/storage/5UGUPJMD/Mak et Joseph - 2018 - Support points.pdf:application/pdf},
}

@article{joseph_split_2022,
	title = {{SPlit}: {An} {Optimal} {Method} for {Data} {Splitting}},
	volume = {64},
	issn = {0040-1706},
	shorttitle = {{SPlit}},
	url = {https://doi.org/10.1080/00401706.2021.1921037},
	doi = {10.1080/00401706.2021.1921037},
	abstract = {In this article, we propose an optimal method referred to as SPlit for splitting a dataset into training and testing sets. SPlit is based on the method of support points (SP), which was initially developed for finding the optimal representative points of a continuous distribution. We adapt SP for subsampling from a dataset using a sequential nearest neighbor algorithm. We also extend SP to deal with categorical variables so that SPlit can be applied to both regression and classification problems. The implementation of SPlit on real datasets shows substantial improvement in the worst-case testing performance for several modeling methods compared to the commonly used random splitting procedure.},
	number = {2},
	urldate = {2023-02-15},
	journal = {Technometrics},
	author = {Joseph, V. Roshan and Vakayil, Akhil},
	month = apr,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2021.1921037},
	keywords = {Cross-validation, Quasi-Monte Carlo, Testing, Training, Validation},
	pages = {166--176},
	file = {Full Text PDF:/home/ekr/Zotero/storage/YA29N3TY/Joseph et Vakayil - 2022 - SPlit An Optimal Method for Data Splitting.pdf:application/pdf},
}

@article{joseph_optimal_2022,
	title = {Optimal ratio for data splitting},
	volume = {15},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11583},
	doi = {10.1002/sam.11583},
	abstract = {It is common to split a dataset into training and testing sets before fitting a statistical or machine learning model. However, there is no clear guidance on how much data should be used for training and testing. In this article, we show that the optimal training/testing splitting ratio is p:1\$\$ {\textbackslash}sqrtp:1 \$\$, where p\$\$ p \$\$ is the number of parameters in a linear regression model that explains the data well.},
	language = {en},
	number = {4},
	urldate = {2023-02-15},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Joseph, V. Roshan},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11583},
	keywords = {testing, training, validation},
	pages = {531--538},
	file = {Snapshot:/home/ekr/Zotero/storage/EASBXNBQ/sam.html:text/html;Full Text PDF:/home/ekr/Zotero/storage/9CXHYC28/Joseph - 2022 - Optimal ratio for data splitting.pdf:application/pdf},
}

@misc{brownlee_what_2017,
	title = {What is the {Difference} {Between} {Test} and {Validation} {Datasets}?},
	url = {https://machinelearningmastery.com/difference-test-validation-datasets/},
	abstract = {A validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters. The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased […]},
	language = {en-US},
	urldate = {2023-02-14},
	journal = {MachineLearningMastery.com},
	author = {Brownlee, Jason},
	month = jul,
	year = {2017},
	file = {Snapshot:/home/ekr/Zotero/storage/QDMPPKQE/difference-test-validation-datasets.html:text/html},
}

@book{r_core_team_r_2021,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2021},
}

@book{johnson_continuous_1995,
	address = {New York [etc},
	edition = {2nd ed.},
	series = {Wiley series in probability and mathematical statistics {Applied} probability and statistics},
	title = {Continuous univariate distributions, volume 2},
	volume = {2},
	isbn = {978-0-471-58494-0},
	language = {eng},
	publisher = {John Wiley \& sons},
	author = {Johnson, Norman Lloyd},
	year = {1995},
	keywords = {Distribution (théorie des probabilités)},
}

@article{porter_hyphal_2022,
	title = {Hyphal systems and their effect on the mechanical properties of fungal sporocarps},
	volume = {145},
	issn = {1742-7061},
	url = {https://www.sciencedirect.com/science/article/pii/S1742706122002161},
	doi = {10.1016/j.actbio.2022.04.011},
	abstract = {Little is known about the mechanical and material properties of hyphae, the single constituent material of Agaricomycetes fungi, despite a growing interest in fungus-based materials. In the Agaricomycetes (the mushrooms and allies), there are three types of hyphae that make up sporocarps: generative, skeletal, and ligative. All filamentous Agaricomycetes can be categorized into one of three categories of hyphal systems that compose them: monomitic, dimitic, and trimitic. Monomitic systems have only generative hyphae. Dimitic systems have generative and either skeletal (most common) or ligative. Trimitic systems are composed of all three kinds of hyphae. SEM imaging, compression testing, and theoretical modeling were used to characterize the material and mechanical properties of representative monomitic, dimitic, and trimitic sporocarps. Compression testing revealed an increase in the compression modulus and compressive strength with the addition of more hyphal types (monomitic to dimitic and dimitic to trimitic). The mesostructure of the trimitic sporocarp was tested and modeled, suggesting that the difference in properties between the solid material and the microtubule mesostructure is a result of differences in structure and not material. Theoretical modeling was completed to estimate the mechanical properties of the individual types of hyphae and showed that skeletal hyphae make the largest contribution to mechanical properties of fungal sporocarps. Understanding the contributions of the different types of hyphae may help in the design and application of fungi-based or bioinspired materials.
Statement of Significance
This research studies the material and mechanical properties of fungal sporocarps and their hyphae, the single constituent material of Agaricomycetes fungi. Though some work has been done on fungal hyphae, this research studies hyphae in context of the three hyphal systems found in Agaricomycetes fungi and estimates the properties of the hyphal filaments, which has not been done previously. This characterization was performed by analyzing the structures and mechanical properties of fungal sporocarps and calculating the theoretical mechanical properties of their hyphae. This data and the resulting conclusions may lead to a better design and implementation process of fungi-based materials in various applications using the properties now known or calculated.},
	language = {en},
	urldate = {2023-02-11},
	journal = {Acta Biomaterialia},
	author = {Porter, Debora Lyn and Naleway, Steven E.},
	month = jun,
	year = {2022},
	keywords = {Fungi, Hyphae, Material science, Mechanical properties},
	pages = {272--282},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/QSPG4PXT/S1742706122002161.html:text/html},
}

@article{money_insights_2008,
	series = {Hyphal {Tip} {Growth}},
	title = {Insights on the mechanics of hyphal growth},
	volume = {22},
	issn = {1749-4613},
	url = {https://www.sciencedirect.com/science/article/pii/S1749461308000195},
	doi = {10.1016/j.fbr.2008.05.002},
	abstract = {This article reviews recent progress in understanding the mechanics of hyphal growth. The pressurization of hyphae by osmosis is often considered an important feature of the tip growth process. But although the hydrostatic pressure within the cytoplasm smooths the expanding surface of these cells, there is little evidence that high levels of turgor are necessary for growth until the hypha encounters friction from its surroundings. Research on movement and growth processes in other kinds of cells, including amoebae and pollen tubes, has done a great deal to inform recent studies on hyphae. Experiments on pressure waves and rhythmic or pulsatile growth are particularly significant and are discussed in relation to the erratic extension rate of hyphae. Significant findings have also come from research on the hyphal cytoskeleton and on the biomechanics of invasive growth. It has been clear for some time that turgor powers the propulsion of hyphae through solid materials and experiments using new techniques have quantified the relevant forces.},
	language = {en},
	number = {2},
	urldate = {2023-02-11},
	journal = {Fungal Biology Reviews},
	author = {Money, Nicholas P.},
	month = may,
	year = {2008},
	keywords = {Cytoskeleton, Invasive growth, Pollen tubes, Pulsatile growth, Tip growth, Turgor pressure},
	pages = {71--76},
	file = {ScienceDirect Snapshot:/home/ekr/Zotero/storage/UWBYPAQB/S1749461308000195.html:text/html;ScienceDirect Full Text PDF:/home/ekr/Zotero/storage/EJAMHCS2/Money - 2008 - Insights on the mechanics of hyphal growth.pdf:application/pdf},
}

@article{schlimmer_mushroom_1987,
	edition = {University of California},
	title = {Mushroom {Data} {Set}},
	url = {https://archive.ics.uci.edu/ml/datasets/Mushroom},
	language = {English},
	author = {Schlimmer, Jeff},
	year = {1987},
}

@book{kuhn_caret_nodate,
	title = {The caret {Package}},
	url = {https://topepo.github.io/caret/index.html},
	abstract = {Documentation for the caret package.},
	urldate = {2022-12-11},
	author = {Kuhn, Max},
	file = {Snapshot:/home/ekr/Zotero/storage/8GNIME6R/index.html:text/html},
}

@article{bergstra_random_nodate,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	pages = {25},
	file = {Bergstra et Bengio - Random Search for Hyper-Parameter Optimization.pdf:/home/ekr/Zotero/storage/2M2SGP7Z/Bergstra et Bengio - Random Search for Hyper-Parameter Optimization.pdf:application/pdf},
}

@book{gramacy_surrogates_nodate,
	title = {Surrogates},
	url = {https://bookdown.org/rbg/surrogates/#front-cover},
	abstract = {Surrogates: a new graduate level textbook on topics lying at the interface between machine learning, spatial statistics, computer simulation, meta-modeling (i.e., emulation), and design of experiments. Gaussian process emphasis facilitates flexible nonparametric and nonlinear modeling, with applications to uncertainty quantification, sensitivity analysis, calibration of computer models to field data, sequential design and (blackbox) optimization under uncertainty. Presentation targets numerically competent scientists in the engineering, physical, and biological sciences. Treatment includes historical perspective and canonical examples, but primarily concentrates on modern statistical methods, computation and implementation in R at modern scale. Rmarkdown facilitates a fully reproducible tour complete with motivation from, application to, and illustration with, compelling real-data examples.},
	urldate = {2022-12-11},
	author = {Gramacy, Robert B.},
	file = {Snapshot:/home/ekr/Zotero/storage/GJP5VIHW/surrogates.html:text/html},
}

@article{wagner_mushroom_2021,
	title = {Mushroom data creation, curation, and simulation to support classification tasks},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-87602-3},
	doi = {10.1038/s41598-021-87602-3},
	abstract = {Predicting if a set of mushrooms is edible or not corresponds to the task of classifying them into two groups—edible or poisonous—on the basis of a classification rule. To support this binary task, we have collected the largest and most comprehensive attribute based data available. In this work, we detail the creation, curation and simulation of a data set for binary classification. Thanks to natural language processing, the primary data are based on a text book for mushroom identification and contain 173 species from 23 families. While the secondary data comprise simulated or hypothetical entries that are structurally comparable to the 1987 data, it serves as pilot data for classification tasks. We evaluated different machine learning algorithms, namely, naive Bayes, logistic regression, and linear discriminant analysis (LDA), and random forests (RF). We found that the RF provided the best results with a five-fold Cross-Validation accuracy and F2-score of 1.0 (\$\${\textbackslash}mu =1\$\$, \$\${\textbackslash}sigma =0\$\$), respectively. The results of our pilot are conclusive and indicate that our data were not linearly separable. Unlike the 1987 data which showed good results using a linear decision boundary with the LDA. Our data set contains 23 families and is the largest available. We further provide a fully reproducible workflow and provide the data under the FAIR principles.},
	language = {en},
	number = {1},
	urldate = {2022-12-10},
	journal = {Scientific Reports},
	author = {Wagner, Dennis and Heider, Dominik and Hattab, Georges},
	month = apr,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Classification and taxonomy, Computational models, Data integration, Data mining, Data processing, Machine learning, Quality control, Scientific data},
	pages = {8134},
	file = {Full Text PDF:/home/ekr/Zotero/storage/68TDVMV2/Wagner et al. - 2021 - Mushroom data creation, curation, and simulation t.pdf:application/pdf},
}

@misc{courtecuisse_initiation_2020,
	title = {Initiation à la reconnaissance des champignons du {Nord} de la {France} - {Clé} pour la détermination des espèces les plus fréquentes},
	shorttitle = {Clé de détermination des champignons du {Nord} de la {France}},
	language = {français},
	publisher = {Département des Sciences Végétales et Fongiques, Faculté de Pharmacie de Lille},
	author = {Courtecuisse, Régis and Moreau, Pierre-Arthur and Welti, Stéphane},
	year = {2020},
}

@book{courtecuisse_champignons_2013,
	series = {Guides {Delachaux}},
	title = {Champignons de {France} et d'{Europe}},
	isbn = {978-2-603-02038-8},
	language = {français},
	publisher = {Delachaux et Niestlé},
	author = {Courtecuisse, Régis and Duhem, Bernard},
	year = {2013},
}

@book{courtecuisse_cle_1986,
	title = {Clé de détermination macroscopique des champignons supérieurs des régions du {Nord} de la {France}},
	language = {français},
	publisher = {Société mycologique du Nord de la France},
	author = {Courtecuisse, Régis},
	year = {1986},
}

@article{santiago_construction_2012,
	title = {Construction of space-filling designs using {WSP} algorithm for high dimensional spaces},
	volume = {113},
	issn = {01697439},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169743911001195},
	doi = {10.1016/j.chemolab.2011.06.003},
	abstract = {In the computer experiments setting, if the relationship between the response and the inputs is unknown, then the purpose is to use designs that spread the points at which the response is observed evenly throughout the region. These designs are called space-ﬁlling designs (SFD) and the most known are Latin Hypercubes (random, orthogonal, optimized) and low discrepancy sequences. But, simulation codes becoming more and more complex, high dimensional optimal designs are needed to study a high number of parameters (more than 20 parameters) and the construction proves difﬁcult. The aim of this study is to explore a construction method of new space-ﬁlling designs for high dimensional spaces. After a short presentation of the criteria considered to quantify the intrinsic quality of the designs, the generation of these designs using WSP algorithm is presented. As the ﬁrst step consists in generating candidate points, the inﬂuence of the initial set of points is investigated in dimension 20 and the ﬁnal designs are compared with others space-ﬁlling designs. Then, designs are proposed in dimension 20, 30, 40 and 50 and the study of the intrinsic quality of these new space-ﬁlling designs highlights the robustness of this generation method in high dimensional spaces.},
	language = {en},
	urldate = {2023-03-04},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Santiago, J. and Claeys-Bruno, M. and Sergent, M.},
	month = apr,
	year = {2012},
	pages = {26--31},
	file = {Santiago et al. - 2012 - Construction of space-filling designs using WSP al.pdf:/home/ekr/Zotero/storage/K4BDP67I/Santiago et al. - 2012 - Construction of space-filling designs using WSP al.pdf:application/pdf},
}

@article{youden_index_1950,
	title = {Index for rating diagnostic tests},
	volume = {3},
	issn = {1097-0142},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-0142%281950%293%3A1%3C32%3A%3AAID-CNCR2820030106%3E3.0.CO%3B2-3},
	doi = {10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3},
	language = {en},
	number = {1},
	urldate = {2023-03-04},
	journal = {Cancer},
	author = {Youden, W. J.},
	year = {1950},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-0142\%281950\%293\%3A1\%3C32\%3A\%3AAID-CNCR2820030106\%3E3.0.CO\%3B2-3},
	pages = {32--35},
	file = {Full Text PDF:/home/ekr/Zotero/storage/SYG6VC9F/Youden - 1950 - Index for rating diagnostic tests.pdf:application/pdf;Snapshot:/home/ekr/Zotero/storage/KEGXCE5R/1097-0142(1950)3132AID-CNCR28200301063.0.html:text/html},
}

@article{rucker_summary_2010,
	title = {Summary {ROC} curve based on a weighted {Youden} index for selecting an optimal cutpoint in meta-analysis of diagnostic accuracy},
	volume = {29},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3937},
	doi = {10.1002/sim.3937},
	abstract = {Established approaches for analyzing meta-analyses of diagnostic accuracy model the bivariate distribution of the observed pairs of specificity Sp and sensitivity Se, thus accounting for across-study correlation. However, it is still a matter of debate how to define a summary ROC (SROC) curve. It was recently pointed out that the SROC curve is in principle unidentifiable if only one (Sp, Se) pair per study is known. We evaluate an alternative approach, modeling the study-specific ROC curves based on the assumption of linearity in logit space. A setting is considered in which the pair (Sp, Se) that is selected for publication in a particular study maximizes a weighted Youden index λSe+(1−λ)Sp with a given weight λ.This leads to a fixed slope (1−λ)/λ of the ROC curve in (1−Sp, Se), equivalent to a slope of (1−λ)Sp(1−Sp)/(λSe(1−Se)) for the corresponding straight line in logit space. While the slope depends on the variance ratio of the underlying distributions, the intercept is a function of the mean difference. Our approach leads in a natural way to a new, model-based proposal for a summary ROC curve. It is illustrated using an example from the literature. Copyright © 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {30},
	urldate = {2023-03-04},
	journal = {Statistics in Medicine},
	author = {Rücker, Gerta and Schumacher, Martin},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3937},
	keywords = {meta-analysis, diagnostic accuracy, summary ROC curve, Youden index},
	pages = {3069--3078},
	file = {Full Text PDF:/home/ekr/Zotero/storage/YFIIMPHV/Rücker et Schumacher - 2010 - Summary ROC curve based on a weighted Youden index.pdf:application/pdf;Snapshot:/home/ekr/Zotero/storage/DCUV5IPC/sim.html:text/html},
}

@article{cohen_coefficient_1960,
	title = {A {Coefficient} of {Agreement} for {Nominal} {Scales}},
	volume = {20},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316446002000104},
	doi = {10.1177/001316446002000104},
	language = {en},
	number = {1},
	urldate = {2023-03-12},
	journal = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	month = apr,
	year = {1960},
	note = {Publisher: SAGE Publications Inc},
	pages = {37--46},
	file = {SAGE PDF Full Text:/home/ekr/Zotero/storage/5HIYB5TA/Cohen - 1960 - A Coefficient of Agreement for Nominal Scales.pdf:application/pdf},
}

@article{landis_measurement_1977,
	title = {The {Measurement} of {Observer} {Agreement} for {Categorical} {Data}},
	volume = {33},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2529310},
	doi = {10.2307/2529310},
	abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
	number = {1},
	urldate = {2023-03-12},
	journal = {Biometrics},
	author = {Landis, J. Richard and Koch, Gary G.},
	year = {1977},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {159--174},
	file = {JSTOR Full Text PDF:/home/ekr/Zotero/storage/YR5TJ3AJ/Landis et Koch - 1977 - The Measurement of Observer Agreement for Categori.pdf:application/pdf},
}

@article{vakayil_data_2022,
	title = {Data {Twinning}},
	volume = {15},
	issn = {1932-1872},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11574},
	doi = {10.1002/sam.11574},
	abstract = {In this work, we develop a method named Twinning for partitioning a dataset into statistically similar twin sets. Twinning is based on SPlit, a recently proposed model-independent method for optimally splitting a dataset into training and testing sets. Twinning is orders of magnitude faster than the SPlit algorithm, which makes it applicable to Big Data problems such as data compression. Twinning can also be used for generating multiple splits of a given dataset to aid divide-and-conquer procedures and k-fold cross validation.},
	language = {en},
	number = {5},
	urldate = {2023-03-12},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Vakayil, Akhil and Joseph, V. Roshan},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11574},
	keywords = {testing, training, validation, data compression, data splitting},
	pages = {598--610},
	file = {Full Text PDF:/home/ekr/Zotero/storage/DI76S66Z/Vakayil et Joseph - 2022 - Data Twinning.pdf:application/pdf;Snapshot:/home/ekr/Zotero/storage/3AUY3WSM/sam.html:text/html},
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2023-04-25},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
	pages = {179--188},
	file = {Snapshot:/home/ekr/Zotero/storage/LFS4J77K/j.1469-1809.1936.tb02137.html:text/html;Full Text PDF:/home/ekr/Zotero/storage/H5CKJQML/Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf},
}

@article{anderson_r_1996,
	title = {R. {A}. {Fisher} and {Multivariate} {Analysis}},
	volume = {11},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/2246198},
	abstract = {This paper reviews R. A. Fisher's many fundamental contributions to multivariate statistical analysis--from the derivation of the distribution of the sample correlation coefficient to discriminant analysis. The emphasis here is on the conceptual and mathematical development. All of his papers on multivariate analysis will be included in this survey.},
	number = {1},
	urldate = {2023-04-25},
	journal = {Statistical Science},
	author = {Anderson, T. W.},
	year = {1996},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {20--34},
	file = {JSTOR Full Text PDF:/home/ekr/Zotero/storage/KZW8MRZP/Anderson - 1996 - R. A. Fisher and Multivariate Analysis.pdf:application/pdf},
}

@incollection{izenman_linear_2008,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Linear {Dimensionality} {Reduction}},
	isbn = {978-0-387-78189-1},
	url = {https://doi.org/10.1007/978-0-387-78189-1_7},
	abstract = {When faced with situations involving high-dimensional data, it is natural to consider the possibility of projecting those data onto a lower-dimensional subspace without losing important information regarding some characteristic of the original variables. One way of accomplishing this reduction of dimensionality is through variable selection, also called feature selection (see Section 5.7). Another way is by creating a reduced set of linear or nonlinear transformations of the input variables. The creation of such composite variables (or features) by projection methods is often referred to as feature extraction. Usually, we wish to find those low-dimensional projections of the input data that enjoy some sort of optimality properties.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Modern {Multivariate} {Statistical} {Techniques}: {Regression}, {Classification}, and {Manifold} {Learning}},
	publisher = {Springer},
	author = {Izenman, Alan Julian},
	editor = {Izenman, Alan J.},
	year = {2008},
	doi = {10.1007/978-0-387-78189-1_7},
	pages = {195--236},
	file = {Full Text PDF:/home/ekr/Zotero/storage/FTZ5Y3C6/Izenman - 2008 - Linear Dimensionality Reduction.pdf:application/pdf},
}

@incollection{izenman_linear_2008-1,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Linear {Discriminant} {Analysis}},
	isbn = {978-0-387-78189-1},
	url = {https://doi.org/10.1007/978-0-387-78189-1_8},
	abstract = {Suppose we are given a learning set \$\${\textbackslash}mathcal\{L\}\$\$of multivariate observations (i.e., input values \$\${\textbackslash}mathfrak\{R\}{\textasciicircum}r\$\$), and suppose each observation is known to have come from one of K predefined classes having similar characteristics. These classes may be identified, for example, as species of plants, levels of credit worthiness of customers, presence or absence of a specific medical condition, different types of tumors, views on Internet censorship, or whether an e-mail message is spam or non-spam.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Modern {Multivariate} {Statistical} {Techniques}: {Regression}, {Classification}, and {Manifold} {Learning}},
	publisher = {Springer},
	author = {Izenman, Alan Julian},
	editor = {Izenman, Alan J.},
	year = {2008},
	doi = {10.1007/978-0-387-78189-1_8},
	pages = {237--280},
	file = {Full Text PDF:/home/ekr/Zotero/storage/RTEQ6CVL/Izenman - 2008 - Linear Discriminant Analysis.pdf:application/pdf},
}

@book{izenman_modern_2008,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Modern {Multivariate} {Statistical} {Techniques}},
	isbn = {978-0-387-78188-4 978-0-387-78189-1},
	url = {http://link.springer.com/10.1007/978-0-387-78189-1},
	urldate = {2023-04-23},
	publisher = {Springer},
	author = {Izenman, Alan J.},
	editor = {Casella, G. and Fienberg, S. and Olkin, I.},
	year = {2008},
	doi = {10.1007/978-0-387-78189-1},
	keywords = {Random Forest, Support Vector Machine, Boosting, bootstrap aggregating, cluster analysis, Clustering, data analysis, data mining, Factor analysis, Latent variable model, Linear discriminant analysis, machine learning, Mathematica, multidimensional scaling, Projection pursuit},
	file = {Full Text PDF:/home/ekr/Zotero/storage/JP6QV7HC/Izenman - 2008 - Modern Multivariate Statistical Techniques.pdf:application/pdf},
}

@incollection{xanthopoulos_support_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Support {Vector} {Machines}},
	isbn = {978-1-4419-9878-1},
	url = {https://doi.org/10.1007/978-1-4419-9878-1_5},
	abstract = {In this chapter we describe one of the most successful supervised learning algorithms namely suppor vector machines (SVMs). The SVM is one of the conceptually simplest algorithms whereas at the same time one of the best especially for binary classification. Here we illustrate the mathematical formulation of SVM together with its robust equivalent for the most common uncertainty sets.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Robust {Data} {Mining}},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	editor = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1_5},
	keywords = {Admissible Kernel Function, Hard Margin Classiﬁer, Optimal Separating Hyperplane, Robust Equivalent, Super Vector Machine (SVM)},
	pages = {35--48},
	file = {Full Text PDF:/home/ekr/Zotero/storage/6JQ763CD/Xanthopoulos et al. - 2013 - Support Vector Machines.pdf:application/pdf},
}

@incollection{xanthopoulos_linear_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Linear {Discriminant} {Analysis}},
	isbn = {978-1-4419-9878-1},
	url = {https://doi.org/10.1007/978-1-4419-9878-1_4},
	abstract = {In this chapter we discuss another popular data mining algorithm that can be used for supervised or unsupervised learning. Linear Discriminant Analysis (LDA) was proposed by R. Fischer in 1936. It consists in finding the projection hyperplane that minimizes the interclass variance and maximizes the distance between the projected means of the classes. Similarly to PCA, these two objectives can be solved by solving an eigenvalue problem with the corresponding eigenvector defining the hyperplane of interest. This hyperplane can be used for classification, dimensionality reduction and for interpretation of the importance of the given features. In the first part of the chapter we discuss the generic formulation of LDA whereas in the second we present the robust counterpart scheme originally proposed by Kim and Boyd. We also discuss the non linear extension of LDA through the kernel transformation.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Robust {Data} {Mining}},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	editor = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1_4},
	keywords = {Linear Discriminant Analysis, Data Mining Algorithm, Feature Space, Kernel Trick, Robust Counterpart},
	pages = {27--33},
	file = {Full Text PDF:/home/ekr/Zotero/storage/YQCTDJHK/Xanthopoulos et al. - 2013 - Linear Discriminant Analysis.pdf:application/pdf},
}

@incollection{xanthopoulos_principal_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Principal {Component} {Analysis}},
	isbn = {978-1-4419-9878-1},
	url = {https://doi.org/10.1007/978-1-4419-9878-1_3},
	abstract = {The principal component analysis (PCA) transformation is a very common and well-studied data analysis technique that aims to identify some linear trends and simple patterns in a group of samples. It has application in several areas of engineering. It is popular from computational perspective as it requires only an eigendecomposition or singular value decomposition. There are two alternative optimization approaches for obtaining principal component analysis solution, the one of variance maximization and the one of minimum error formulation. Both start with a “different” initial objective and end up providing the same solution. It is necessary to study and understand both of these alternative approaches. In the second part of this chapter we present the robust counterpart formulation of PCA and demonstrate how such a formulation can be used in practice in order to produce sparse solutions.},
	language = {en},
	urldate = {2023-04-23},
	booktitle = {Robust {Data} {Mining}},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	editor = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1_3},
	keywords = {Robust Optimization, Robust Counterpart, Principal Component Analysis, Robust Principal Component Analysis, Sparse Solution},
	pages = {21--26},
	file = {Full Text PDF:/home/ekr/Zotero/storage/R4TPCTFD/Xanthopoulos et al. - 2013 - Principal Component Analysis.pdf:application/pdf},
}

@book{xanthopoulos_robust_2013,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Optimization}},
	title = {Robust {Data} {Mining}},
	isbn = {978-1-4419-9877-4 978-1-4419-9878-1},
	url = {https://link.springer.com/10.1007/978-1-4419-9878-1},
	language = {en},
	urldate = {2023-04-23},
	publisher = {Springer},
	author = {Xanthopoulos, Petros and Pardalos, Panos M. and Trafalis, Theodore B.},
	year = {2013},
	doi = {10.1007/978-1-4419-9878-1},
	keywords = {linear discriminant analysis, robust data mining, robust optimization, support vector machines},
	file = {Full Text PDF:/home/ekr/Zotero/storage/HK7EHCZ4/Xanthopoulos et al. - 2013 - Robust Data Mining.pdf:application/pdf},
}

@misc{noauthor_linear_nodate,
	title = {Linear {Discriminant} {Analysis}, {Explained} {\textbar} by {YANG} {Xiaozhou} {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b},
	urldate = {2023-04-23},
	file = {Linear Discriminant Analysis, Explained | by YANG Xiaozhou | Towards Data Science:/home/ekr/Zotero/storage/VWPXALMB/linear-discriminant-analysis-explained-f88be6c1e00b.html:text/html},
}

@book{hastie_elements_2016,
	edition = {2nd},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning} - {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	language = {en},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2016},
	file = {Tibshirani et Friedman - Valerie and Patrick Hastie.pdf:/home/ekr/Zotero/storage/5PN8P6WI/Tibshirani et Friedman - Valerie and Patrick Hastie.pdf:application/pdf},
}
